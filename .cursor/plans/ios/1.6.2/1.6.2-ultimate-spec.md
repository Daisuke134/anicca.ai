# Anicca 1.6.2 Ultimate Implementation Specification

> **ç›®çš„**: å®Ÿè£…è€…ãŒè³ªå•ãªã—ã§å®Œå…¨å®Ÿè£…ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã®è©³ç´°ä»•æ§˜
> **RFC 2119 æº–æ‹ **: MUST, SHOULD, MAY ã‚’ä½¿ç”¨
> **æœ€çµ‚æ›´æ–°**: 2026-02-03
> **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: 4ã¤ã®å°‚é–€ãƒªã‚µãƒ¼ãƒï¼ˆMoltbot, memU, Anthropic, é€šçŸ¥ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼‰ã‚’çµ±åˆ

---

## 0. Executive Summary

| é …ç›® | å†…å®¹ |
|------|------|
| **ã‚´ãƒ¼ãƒ«** | 24/7ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ä»æ•™ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã—ã¦ã€è‹¦ã—ã¿ã®ã‚ã‚‹å ´æ‰€ã«è‡ªã‚‰è¡Œãã€Nudgeã™ã‚‹ |
| **ã‚³ã‚¢ãƒ‘ã‚¿ãƒ¼ãƒ³** | Gatewayåˆ¶å¾¡ãƒ—ãƒ¬ãƒ¼ãƒ³ + 3å±¤ãƒ¡ãƒ¢ãƒª + Workflowå„ªå…ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ |
| **ãƒ‡ãƒ—ãƒ­ã‚¤** | Hetzner VPS (Tier A: Docker + Tailscale) |
| **ã‚³ã‚¹ãƒˆç›®æ¨™** | LLMãƒˆãƒ¼ã‚¯ãƒ³90%å‰Šæ¸›ï¼ˆmemUãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨ï¼‰ |
| **å“è³ªç›®æ¨™** | æŠ•ç¨¿ã‚¹ã‚³ã‚¢ >= 7/10 ã®ã¿å…¬é–‹ |

---

## 1. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ±ºå®šè¨˜éŒ² (ADR)

### ADR-001: Workflows vs Agents

**æ±ºå®š**: Workflowå„ªå…ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨

| åŸºæº– | Workflow | Agent | Aniccaé¸æŠ |
|------|----------|-------|------------|
| **äºˆæ¸¬å¯èƒ½æ€§** | é«˜ï¼ˆäº‹å‰å®šç¾©ãƒ‘ã‚¹ï¼‰ | ä½ï¼ˆå‹•çš„åˆ¤æ–­ï¼‰ | âœ… Workflow |
| **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·** | ä½ | é«˜ | âœ… Workflow |
| **ã‚³ã‚¹ãƒˆ** | ä½ | é«˜ | âœ… Workflow |
| **ãƒ‡ãƒãƒƒã‚°å®¹æ˜“æ€§** | é«˜ | ä½ | âœ… Workflow |
| **æŸ”è»Ÿæ€§** | ä½ | é«˜ | AgentãŒæœ‰åˆ©ã ãŒä¸è¦ |

**æ ¹æ‹ **: Anthropic "Building Effective Agents" ã‚ˆã‚Šã€Œå¯èƒ½ãªé™ã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªè§£æ±ºç­–ã‹ã‚‰å§‹ã‚ã‚‹ã€

**ä¾‹å¤–**: ä»¥ä¸‹ã®å ´åˆã®ã¿Agentä½¿ç”¨
- è‹¦ã—ã¿æŠ•ç¨¿ã¸ã®è¿”ä¿¡ç”Ÿæˆï¼ˆæ–‡è„ˆä¾å­˜æ€§ãŒé«˜ã„ï¼‰
- ãƒ¬ãƒ“ãƒ¥ãƒ¼è¿”ä¿¡ç”Ÿæˆï¼ˆé¡§å®¢å¯¾å¿œã®ç¹Šç´°ã•ï¼‰

### ADR-002: Cron vs Heartbeat

**æ±ºå®š**: ã‚¿ã‚¹ã‚¯ç¨®åˆ¥ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘

| ã‚¿ã‚¹ã‚¯ç¨®åˆ¥ | ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ | ç†ç”± |
|-----------|-------------|------|
| **x-poster** | Cron (isolated) | æ­£ç¢ºãªã‚¿ã‚¤ãƒŸãƒ³ã‚°å¿…é ˆã€æ–‡è„ˆä¸è¦ |
| **tiktok-poster** | Cron (isolated) | åŒä¸Š |
| **trend-hunter** | Cron (interval) | 4æ™‚é–“é–“éš”ã€ç‹¬ç«‹å®Ÿè¡Œ |
| **feedback-fetch** | Cron (interval) | 4æ™‚é–“é–“éš”ã€ç‹¬ç«‹å®Ÿè¡Œ |
| **suffering-detector** | Heartbeat | æœ€æ–°ã®ä¼šè©±æ–‡è„ˆãŒå¿…è¦ |
| **moltbook-responder** | Heartbeat | ä¼šè©±ç¶™ç¶šæ€§ãŒå¿…è¦ |

**Moltbotç ”ç©¶ã‹ã‚‰ã®åˆ¤æ–­åŸºæº–**:
- æ­£ç¢ºãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ãŒå¿…è¦ â†’ Cron (isolated)
- ä¼šè©±æ–‡è„ˆãŒå¿…è¦ â†’ Heartbeat (main session)
- ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«/è¨­å®šãŒå¿…è¦ â†’ Cron (isolated)
- ãƒãƒ£ãƒ³ãƒãƒ«é…ä¿¡ãŒå¿…è¦ â†’ Cron with delivery

### ADR-003: ãƒ¡ãƒ¢ãƒªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**æ±ºå®š**: memU 3å±¤éšå±¤ã‚’æ¡ç”¨

```
memory/
â”œâ”€â”€ categories/              â† Category Layerï¼ˆè¦ç´„ãƒ»çµ±åˆï¼‰
â”‚   â”œâ”€â”€ preferences/
â”‚   â”‚   â”œâ”€â”€ morning_nudges.md
â”‚   â”‚   â””â”€â”€ tone_preferences.md
â”‚   â””â”€â”€ behaviors/
â”‚       â”œâ”€â”€ engagement_patterns.md
â”‚       â””â”€â”€ timing_responses.md
â”œâ”€â”€ items/                   â† Item Layerï¼ˆç´°ç²’åº¦ãƒ¡ãƒ¢ãƒªï¼‰
â”‚   â”œâ”€â”€ item_001.json       # "strict tone at 6am gets thumbs_up"
â”‚   â””â”€â”€ item_002.json       # "gentle tone works for anxiety"
â””â”€â”€ resources/              â† Resource Layerï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
    â”œâ”€â”€ nudge_feedback_123.json
    â””â”€â”€ behavior_event_456.json
```

**ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœ**:
- ç¾åœ¨: æ¯å›å…¨å±¥æ­´ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹ â†’ 100%ã‚³ã‚¹ãƒˆ
- memUå¾Œ: Category â†’ Item â†’ Resource ã®éšå±¤çš„æ¤œç´¢ â†’ 10-20%ã‚³ã‚¹ãƒˆ

---

## 2. ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè©³ç´°è¨­è¨ˆ

### 2.1 Gatewayåˆ¶å¾¡ãƒ—ãƒ¬ãƒ¼ãƒ³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ANICCA GATEWAY (VPS)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      OpenClaw Gateway                                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚ WebSocket Server (ws://127.0.0.1:18789)                         â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Typed protocol (TypeBox schemas)                              â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Session management                                             â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Tool orchestration                                             â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚                                  â”‚                                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚                       SESSION MANAGER                          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                                                â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Sessions:                                                     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ main           â†’ DMå¯¾è©±ï¼ˆHeartbeatã‚¿ã‚¹ã‚¯ï¼‰                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ cron:x-poster  â†’ æŠ•ç¨¿ã‚¸ãƒ§ãƒ–ï¼ˆisolatedï¼‰                     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ cron:trend     â†’ ãƒˆãƒ¬ãƒ³ãƒ‰ç›£è¦–ï¼ˆisolatedï¼‰                   â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ hook:uuid      â†’ Webhookå‡¦ç†ï¼ˆä¸€æ™‚çš„ï¼‰                      â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                                                â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Session Isolation Matrix:                                     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Session         â”‚ Tool Accessâ”‚ Memory      â”‚ Delivery   â”‚  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ main            â”‚ Full       â”‚ Shared      â”‚ WhatsApp   â”‚  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ cron:*          â”‚ Scoped     â”‚ Isolated    â”‚ Optional   â”‚  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ hook:*          â”‚ Minimal    â”‚ None        â”‚ Callback   â”‚  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                  â”‚                                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚                       CRON SCHEDULER                           â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                                                â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Storage: ~/.openclaw/cron/jobs.json                          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  History: ~/.openclaw/cron/runs/<jobId>.jsonl                 â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                                                â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Job Types:                                                    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ at: ä¸€å›é™ã‚Š (ISO 8601 timestamp)                          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ every: å›ºå®šé–“éš” (ms)                                        â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ cron: 5ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å¼ + ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³                         â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                                                â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Execution Modes:                                              â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ systemEvent: main session ã«ã‚¨ãƒ³ã‚­ãƒ¥ãƒ¼                     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ agentTurn: å°‚ç”¨ cron:<jobId> session ã§å®Ÿè¡Œ               â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                      â”‚                                       â”‚
â”‚                                      â†“ ANICCA_AGENT_TOKEN                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Skillsè©³ç´°è¨­è¨ˆ

#### 2.2.1 x-poster Skill (å®Œå…¨ç‰ˆ)

**ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ**:
```
/home/anicca/openclaw/skills/x-poster/
â”œâ”€â”€ skill.yaml           # Skillå®šç¾©ï¼ˆä¸‹è¨˜å‚ç…§ï¼‰
â”œâ”€â”€ main.py              # ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯
â”œâ”€â”€ verifier.py          # ãƒ†ã‚­ã‚¹ãƒˆ/ç”»åƒæ¤œè¨¼
â”œâ”€â”€ hook_selector.py     # Thompson Samplingãƒ™ãƒ¼ã‚¹ãƒ•ãƒƒã‚¯é¸å®š
â”œâ”€â”€ error_handler.py     # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
â”œâ”€â”€ metrics.py           # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
â””â”€â”€ tests/
    â”œâ”€â”€ test_main.py
    â”œâ”€â”€ test_verifier.py
    â””â”€â”€ fixtures/
```

**skill.yaml (å®Œå…¨ç‰ˆ)**:
```yaml
name: x-poster
description: |
  Post wisdom content to X/Twitter with quality verification.
  Uses Thompson Sampling for hook selection and LLM for content verification.
version: 1.0.0
author: anicca

triggers:
  schedule:
    cron:
      - "0 0 * * *"   # 09:00 JST (UTC+9)
      - "0 12 * * *"  # 21:00 JST
    timezone: "Asia/Tokyo"
    
  # Manual trigger support
  manual: true

session:
  target: "isolated"          # Don't pollute main session
  wakeMode: "now"            # Execute immediately when triggered
  isolation:
    postToMainPrefix: "[XæŠ•ç¨¿]"
    postToMainMode: "summary"  # Only post summary to main

env:
  required:
    - ANICCA_AGENT_TOKEN
    - ANICCA_PROXY_BASE_URL
    - BLOTATO_API_KEY
    - X_ACCOUNT_ID
    - OPENAI_API_KEY
  optional:
    - FAL_API_KEY
    - SLACK_WEBHOOK_AGENTS

tools:
  allow:
    - bash           # For simple file operations
    - read           # Read configuration
    - write          # Write logs
  deny:
    - browser        # Not needed, API-based
    - nodes          # Not needed

retry:
  max_attempts: 3
  backoff: "exponential"
  initial_delay_ms: 1000
  max_delay_ms: 30000
  jitter: true

error_handling:
  on_error: "notify_and_abort"
  fallback_content: null       # No fallback, abort if verification fails
  
  # Error classification (from Anthropic research)
  retry_on:
    - 429  # Rate limit
    - 500  # Server error
    - 502  # Bad gateway
    - 503  # Service unavailable
    - 529  # Overloaded
  abort_on:
    - 400  # Bad request (client error)
    - 401  # Unauthorized
    - 403  # Forbidden
    - 404  # Not found

outputs:
  - agent_post_id
  - blotato_post_id
  - text_score
  - image_score
  - hook_used
  - verification_attempts
```

**main.py (å®Œå…¨ç‰ˆ)**:
```python
#!/usr/bin/env python3
"""
x-poster Skill â€” Post to X with quality verification

Architecture: Workflow (not Agent)
- Predictable execution path
- Low latency, low cost
- Easy debugging

Flow:
1. Select best hook using Thompson Sampling
2. Generate text content via /api/agent/content
3. Verify text quality (score >= 7, max 3 attempts)
4. Generate image via fal (optional)
5. Verify image quality (score >= 7, max 3 attempts)
6. Post via Blotato API
7. Save to agent_posts
8. Update hook statistics (for future selection)
9. Notify Slack

Error Handling (from Anthropic research):
- 4xx (except 429): Don't retry
- 429: Exponential backoff with jitter
- 5xx: Exponential backoff, max 3 attempts
- Fallback: Abort and notify
"""
import os
import sys
import json
import time
import random
import logging
from datetime import datetime, timezone, timedelta
from typing import Optional, Dict, Any, Tuple
from dataclasses import dataclass, asdict

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Local imports
from verifier import verify_text, verify_image
from hook_selector import select_hook_thompson
from error_handler import (
    handle_api_error,
    notify_slack,
    ExponentialBackoff,
    DeadLetterQueue
)
from metrics import record_execution_metrics

# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass
class Config:
    """Configuration with validation."""
    api_base_url: str
    agent_token: str
    blotato_api_key: str
    blotato_base_url: str = "https://api.blotato.com/v2"
    x_account_id: str = ""
    fal_api_key: str = ""
    slack_webhook: str = ""
    
    # Verification settings (0-5 scale, threshold 3)
    max_verification_attempts: int = 3
    min_text_score: int = 3   # 0-5 scale, 3+ = pass
    min_image_score: int = 3  # 0-5 scale, 3+ = pass
    
    # Retry settings
    max_retries: int = 3
    retry_backoff_factor: float = 1.0
    retry_status_forcelist: tuple = (429, 500, 502, 503, 529)
    
    # Timezone
    jst: timezone = timezone(timedelta(hours=9))
    
    @classmethod
    def from_env(cls) -> "Config":
        """Create config from environment variables."""
        required = ["ANICCA_PROXY_BASE_URL", "ANICCA_AGENT_TOKEN", "BLOTATO_API_KEY"]
        missing = [k for k in required if not os.environ.get(k)]
        if missing:
            raise ValueError(f"Missing required env vars: {missing}")
        
        return cls(
            api_base_url=os.environ["ANICCA_PROXY_BASE_URL"],
            agent_token=os.environ["ANICCA_AGENT_TOKEN"],
            blotato_api_key=os.environ["BLOTATO_API_KEY"],
            x_account_id=os.environ.get("X_ACCOUNT_ID", ""),
            fal_api_key=os.environ.get("FAL_API_KEY", ""),
            slack_webhook=os.environ.get("SLACK_WEBHOOK_AGENTS", ""),
        )


# ============================================================================
# HTTP CLIENT (with retry)
# ============================================================================

def create_http_session(config: Config) -> requests.Session:
    """Create HTTP session with retry strategy."""
    session = requests.Session()
    
    retry_strategy = Retry(
        total=config.max_retries,
        backoff_factor=config.retry_backoff_factor,
        status_forcelist=config.retry_status_forcelist,
        allowed_methods=["GET", "POST"],
        raise_on_status=False,
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    
    return session


# ============================================================================
# API FUNCTIONS
# ============================================================================

def api_get(
    session: requests.Session,
    config: Config,
    path: str,
    params: Optional[Dict] = None
) -> Dict:
    """GET request to Railway API with error handling."""
    url = f"{config.api_base_url}{path}"
    headers = {"Authorization": f"Bearer {config.agent_token}"}
    
    try:
        resp = session.get(url, headers=headers, params=params, timeout=30)
        handle_api_error(resp, "Railway API GET")
        return resp.json()
    except requests.RequestException as e:
        logging.error(f"API GET failed: {e}")
        raise


def api_post(
    session: requests.Session,
    config: Config,
    path: str,
    data: Dict
) -> Dict:
    """POST request to Railway API with error handling."""
    url = f"{config.api_base_url}{path}"
    headers = {
        "Authorization": f"Bearer {config.agent_token}",
        "Content-Type": "application/json"
    }
    
    try:
        resp = session.post(url, headers=headers, json=data, timeout=60)
        handle_api_error(resp, "Railway API POST")
        return resp.json()
    except requests.RequestException as e:
        logging.error(f"API POST failed: {e}")
        raise


# ============================================================================
# CONTENT GENERATION
# ============================================================================

def generate_content(
    session: requests.Session,
    config: Config,
    hook: Dict,
    feedback: Optional[str] = None
) -> Dict:
    """Generate content via /api/agent/content with optional feedback."""
    data = {
        "topic": hook["content"],
        "problemType": hook.get("problemType", "procrastination"),
        "tone": "gentle",
        "language": "ja",
    }
    
    if feedback:
        data["feedback"] = feedback
    
    result = api_post(session, config, "/api/agent/content", data)
    return result


def generate_image(config: Config, text: str) -> Optional[str]:
    """Generate image via fal API."""
    if not config.fal_api_key:
        return None
    
    prompt = f"""
Create a warm, minimalist illustration for this message:
"{text[:200]}"

Style: Soft colors, simple shapes, calming, no text in image.
Mood: Supportive, gentle, Buddhist-inspired.
"""
    
    try:
        resp = requests.post(
            "https://fal.run/fal-ai/flux/dev",
            headers={"Authorization": f"Key {config.fal_api_key}"},
            json={"prompt": prompt, "image_size": "square_hd"},
            timeout=120,
        )
        resp.raise_for_status()
        return resp.json().get("images", [{}])[0].get("url")
    except Exception as e:
        logging.warning(f"Image generation failed: {e}")
        return None


# ============================================================================
# VERIFICATION LOOP
# ============================================================================

def verify_and_regenerate_text(
    session: requests.Session,
    config: Config,
    hook: Dict,
) -> Tuple[str, int, int]:
    """
    Verify text quality and regenerate if needed.
    
    Returns: (final_text, final_score, attempt_count)
    """
    attempts = 0
    best_text = ""
    best_score = 0
    feedback = None
    
    for attempt in range(config.max_verification_attempts):
        attempts += 1
        
        # Generate content (with feedback if not first attempt)
        content = generate_content(session, config, hook, feedback)
        text = content["formats"]["short"]
        
        # Verify
        result = verify_text(text)
        score = result["score"]
        
        logging.info(f"Text verification attempt {attempts}: score={score}/5")

        if score > best_score:
            best_score = score
            best_text = text

        if score >= config.min_text_score:
            return text, score, attempts

        # Prepare feedback for next attempt (0-5 scale)
        feedback = f"Score was {score}/5. Feedback: {result['feedback']}. Suggestions: {result['suggestions']}"
    
    # Return best attempt even if below threshold
    return best_text, best_score, attempts


def verify_and_regenerate_image(
    config: Config,
    text: str,
) -> Tuple[Optional[str], Optional[int], int]:
    """
    Verify image quality and regenerate if needed.
    
    Returns: (final_url, final_score, attempt_count)
    """
    if not config.fal_api_key:
        return None, None, 0
    
    attempts = 0
    best_url = None
    best_score = 0
    
    for attempt in range(config.max_verification_attempts):
        attempts += 1
        
        try:
            url = generate_image(config, text)
            if not url:
                break
            
            result = verify_image(url, text)
            score = result["score"]
            
            logging.info(f"Image verification attempt {attempts}: score={score}/5")
            
            if score > best_score:
                best_score = score
                best_url = url
            
            if score >= config.min_image_score:
                return url, score, attempts
                
        except Exception as e:
            logging.warning(f"Image generation/verification failed: {e}")
    
    return best_url, best_score if best_url else None, attempts


# ============================================================================
# POSTING
# ============================================================================

def post_to_blotato(
    config: Config,
    text: str,
    image_url: Optional[str] = None
) -> Dict:
    """Post to X via Blotato API."""
    payload = {
        "post": {
            "accountId": config.x_account_id,
            "content": {
                "text": text,
                "mediaUrls": [image_url] if image_url else [],
                "platform": "twitter",
            },
            "target": {"targetType": "twitter"},
        },
    }
    
    resp = requests.post(
        f"{config.blotato_base_url}/posts",
        headers={
            "blotato-api-key": config.blotato_api_key,
            "Content-Type": "application/json"
        },
        json=payload,
        timeout=30,
    )
    resp.raise_for_status()
    return resp.json()


def save_agent_post(
    session: requests.Session,
    config: Config,
    text: str,
    hook: Dict,
    blotato_id: str,
    text_score: int,
    image_score: Optional[int],
    slot: str,
    text_attempts: int,
    image_attempts: int,
) -> Dict:
    """Save post to agent_posts table."""
    data = {
        "platform": "x",
        "content": text,
        "hook": hook["content"],
        "hookId": hook.get("id"),
        "externalPostId": blotato_id,
        "reasoning": json.dumps({
            "textScore": text_score,
            "imageScore": image_score,
            "slot": slot,
            "textAttempts": text_attempts,
            "imageAttempts": image_attempts,
        }),
    }
    
    return api_post(session, config, "/api/agent/posts", data)


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Main execution flow."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s"
    )
    
    start_time = time.time()
    logging.info("=== x-poster Skill START ===")
    
    # Initialize
    config = Config.from_env()
    session = create_http_session(config)
    now = datetime.now(config.jst)
    slot = "morning" if now.hour < 12 else "evening"
    
    # Execution result
    result = {
        "success": False,
        "hook_used": None,
        "text_score": 0,
        "image_score": None,
        "blotato_id": None,
        "error": None,
    }
    
    try:
        # 1. Select hook using Thompson Sampling
        logging.info("[1/7] Selecting hook...")
        hooks = api_get(session, config, "/api/agent/wisdom", {"limit": 20})
        hook = select_hook_thompson(hooks.get("hooks", []))
        result["hook_used"] = hook["content"][:50]
        logging.info(f"  Selected: {hook['content'][:50]}...")
        
        # 2. Generate and verify text
        logging.info("[2/7] Generating and verifying text...")
        text, text_score, text_attempts = verify_and_regenerate_text(
            session, config, hook
        )
        result["text_score"] = text_score
        logging.info(f"  Final text score: {text_score}/5 ({text_attempts} attempts)")
        
        # 3. Check text score threshold
        if text_score < config.min_text_score:
            raise ValueError(
                f"Text verification failed after {text_attempts} attempts. "
                f"Best score: {text_score}, required: {config.min_text_score}"
            )
        
        # 4. Generate and verify image
        logging.info("[3/7] Generating and verifying image...")
        image_url, image_score, image_attempts = verify_and_regenerate_image(
            config, text
        )
        result["image_score"] = image_score
        if image_url:
            logging.info(f"  Final image score: {image_score}/5 ({image_attempts} attempts)")
        else:
            logging.info("  Skipped (no FAL_API_KEY or generation failed)")
        
        # 5. Post via Blotato
        logging.info("[4/7] Posting via Blotato...")
        blotato_result = post_to_blotato(config, text, image_url)
        blotato_id = str(blotato_result.get("postSubmissionId", blotato_result.get("id", "")))
        result["blotato_id"] = blotato_id
        logging.info(f"  Blotato ID: {blotato_id}")
        
        # 6. Save to database
        logging.info("[5/7] Saving to agent_posts...")
        save_agent_post(
            session, config, text, hook, blotato_id,
            text_score, image_score, slot, text_attempts, image_attempts
        )
        
        # 7. Update hook statistics (for Thompson Sampling)
        logging.info("[6/7] Updating hook statistics...")
        api_post(session, config, "/api/agent/feedback", {
            "hookId": hook.get("id"),
            "outcome": "posted",  # Will be updated with engagement later
        })
        
        # 8. Notify Slack
        logging.info("[7/7] Notifying Slack...")
        notify_slack(config.slack_webhook, f"""ğŸ“¤ XæŠ•ç¨¿å®Œäº†
â€¢ Hook: {hook['content'][:50]}...
â€¢ Text Score: {text_score}/5
â€¢ Image: {'âœ… ' + str(image_score) + '/5' if image_score else 'âŒ'}
â€¢ Blotato ID: {blotato_id}
â€¢ Slot: {slot}""")
        
        result["success"] = True
        
    except Exception as e:
        result["error"] = str(e)
        logging.error(f"Execution failed: {e}")
        
        notify_slack(config.slack_webhook, f"""âš ï¸ x-poster å¤±æ•—
â€¢ Error: {str(e)[:200]}
â€¢ Slot: {slot}
â€¢ Elapsed: {time.time() - start_time:.1f}s""")
        
        # Add to Dead Letter Queue for manual review
        dlq = DeadLetterQueue("/home/anicca/openclaw/dlq/x-poster.jsonl")
        dlq.add({
            "timestamp": datetime.now(config.jst).isoformat(),
            "slot": slot,
            "error": str(e),
            "hook": result.get("hook_used"),
        })
        
        sys.exit(1)
    
    finally:
        elapsed = time.time() - start_time
        logging.info(f"=== x-poster Skill END ({elapsed:.1f}s) ===")
        
        # Record metrics
        record_execution_metrics({
            **result,
            "elapsed_seconds": elapsed,
            "slot": slot,
        })
    
    return result


if __name__ == "__main__":
    main()
```

**hook_selector.py (Thompson Sampling with Best Practices)**:
```python
"""
Hook selection using Thompson Sampling (Multi-Armed Bandit).

Best Practices Applied (from research):
1. Weak Initialization + Dynamic Prior - é¡ä¼¼hookã®å®Ÿç¸¾ã‚’åˆæœŸæ¨å®šã«ä½¿ç”¨
2. Discounted Thompson Sampling - å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡æ•°æ¸›è¡°ï¼ˆÎ³=0.95/é€±ï¼‰
3. Novelty Bonus - æ–°è¦hookã«0.1ãƒœãƒ¼ãƒŠã‚¹ã€10å›ä½¿ç”¨å¾Œå‰Šé™¤
4. Recency Bonus - 7æ—¥ä»¥ä¸Šæœªä½¿ç”¨hookã«é¸æŠç¢ºç‡ä¸Šæ˜‡

References:
- Chapelle & Li (2011) "Empirical Evaluation of Thompson Sampling"
- Raj & Kalyani (2017) "Discounted Thompson Sampling"
- Duolingoç ”ç©¶ "Recovering Difference Softmax Algorithm"
"""
import random
import math
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass


@dataclass
class TSConfig:
    """Thompson Sampling configuration."""
    discount_factor: float = 0.95       # Î³: Weekly discount (0.95^week)
    novelty_bonus: float = 0.1          # Bonus for hooks with <10 uses
    novelty_threshold: int = 10         # Uses before novelty bonus removed
    recency_weight: float = 0.3         # Weight for recency in final score
    recency_max_days: int = 7           # Days for max recency bonus
    lookback_days: int = 90             # Only consider last 90 days of data
    min_prior_samples: int = 3          # Min samples for dynamic prior


def sample_beta(alpha: float, beta: float) -> float:
    """Sample from Beta distribution with safety bounds."""
    # Ensure valid parameters (>0)
    alpha = max(0.001, alpha)
    beta = max(0.001, beta)
    return random.betavariate(alpha, beta)


def calculate_discounted_counts(
    history: List[Dict],
    discount_factor: float,
    lookback_days: int,
) -> Tuple[float, float]:
    """
    Calculate discounted success/failure counts.

    Discounted TS formula:
    Î± = 1 + Î£(Î³^t * success_t)
    Î² = 1 + Î£(Î³^t * failure_t)

    Where t = weeks since event, Î³ = discount_factor
    """
    now = datetime.now()
    cutoff = now - timedelta(days=lookback_days)

    alpha = 1.0  # Prior
    beta = 1.0   # Prior

    for event in history:
        event_date = event.get("date")
        if not event_date:
            continue

        try:
            if isinstance(event_date, str):
                event_dt = datetime.fromisoformat(event_date.replace("Z", "+00:00"))
            else:
                event_dt = event_date

            # Skip events older than lookback period
            if event_dt.replace(tzinfo=None) < cutoff:
                continue

            # Calculate weeks since event
            weeks_since = (now - event_dt.replace(tzinfo=None)).days / 7
            discount = discount_factor ** weeks_since

            if event.get("outcome") == "success":
                alpha += discount
            elif event.get("outcome") == "failure":
                beta += discount
        except:
            continue

    return alpha, beta


def get_dynamic_prior(
    hook: Dict,
    all_hooks: List[Dict],
    config: TSConfig,
) -> Tuple[float, float]:
    """
    Calculate dynamic prior based on similar hooks' performance.

    For new hooks, use weighted average of similar hooks' statistics
    instead of uniform prior (1, 1).
    """
    problem_type = hook.get("problemType")

    # Find similar hooks (same problemType)
    similar_hooks = [
        h for h in all_hooks
        if h.get("problemType") == problem_type
        and h.get("id") != hook.get("id")
        and (h.get("successCount", 0) + h.get("failureCount", 0)) >= config.min_prior_samples
    ]

    if not similar_hooks:
        return 1.0, 1.0  # Default uniform prior

    # Calculate weighted average success rate
    total_success = sum(h.get("successCount", 0) for h in similar_hooks)
    total_failure = sum(h.get("failureCount", 0) for h in similar_hooks)
    total = total_success + total_failure

    if total == 0:
        return 1.0, 1.0

    # Use success rate to set weak prior (scale of 2)
    success_rate = total_success / total
    alpha = 1 + success_rate * 2
    beta = 1 + (1 - success_rate) * 2

    return alpha, beta


def select_hook_thompson(
    hooks: List[Dict],
    config: Optional[TSConfig] = None,
) -> Dict:
    """
    Select hook using Discounted Thompson Sampling.

    Algorithm:
    1. For each hook, calculate discounted Î±, Î² from history
    2. Apply dynamic prior for new hooks
    3. Sample from Beta(Î±, Î²)
    4. Add novelty bonus for <10 uses
    5. Add recency bonus for >7 days unused
    6. Select hook with highest combined score

    Args:
        hooks: List of hook candidates with statistics and history
        config: TSConfig for tuning (defaults provided)

    Returns:
        Selected hook
    """
    if not hooks:
        raise ValueError("No hooks available")

    if len(hooks) == 1:
        return hooks[0]

    config = config or TSConfig()
    now = datetime.now()
    scores = []

    for hook in hooks:
        # Get statistics
        success = hook.get("successCount", 0)
        failure = hook.get("failureCount", 0)
        total_uses = success + failure
        last_used = hook.get("lastUsedAt")
        history = hook.get("history", [])

        # 1. Calculate discounted counts (if history available)
        if history:
            alpha, beta = calculate_discounted_counts(
                history,
                config.discount_factor,
                config.lookback_days
            )
        else:
            # 2. Use dynamic prior for new/low-data hooks
            if total_uses < config.min_prior_samples:
                base_alpha, base_beta = get_dynamic_prior(hook, hooks, config)
            else:
                base_alpha, base_beta = 1.0, 1.0

            alpha = base_alpha + success
            beta = base_beta + failure

        # 3. Thompson Sampling score
        ts_score = sample_beta(alpha, beta)

        # 4. Novelty bonus (prefer less-used hooks, removed after threshold)
        novelty_score = 0.0
        if total_uses < config.novelty_threshold:
            novelty_score = config.novelty_bonus * (1 - total_uses / config.novelty_threshold)

        # 5. Recency bonus (prefer hooks not used recently)
        recency_score = 1.0
        if last_used:
            try:
                if isinstance(last_used, str):
                    last_dt = datetime.fromisoformat(last_used.replace("Z", "+00:00"))
                else:
                    last_dt = last_used
                days_since = (now - last_dt.replace(tzinfo=None)).days
                recency_score = min(1.0, days_since / config.recency_max_days)
            except:
                pass

        # 6. Combined score
        final_score = (
            (1 - config.recency_weight) * ts_score +
            config.recency_weight * recency_score +
            novelty_score
        )

        scores.append({
            "score": final_score,
            "hook": hook,
            "debug": {
                "ts_score": ts_score,
                "alpha": alpha,
                "beta": beta,
                "novelty_score": novelty_score,
                "recency_score": recency_score,
            }
        })

    # Sort by score and return best
    scores.sort(key=lambda x: x["score"], reverse=True)
    return scores[0]["hook"]


def update_hook_statistics(
    hook_id: str,
    outcome: str,  # "success" | "failure" | "neutral"
    api_client,
    timestamp: Optional[datetime] = None,
) -> None:
    """
    Update hook statistics after use.

    For Discounted TS, we need to store the timestamp with each event
    so that historical discounting can be applied.
    """
    timestamp = timestamp or datetime.now()

    data = {
        "hookId": hook_id,
        "outcome": outcome,
        "timestamp": timestamp.isoformat(),
    }

    if outcome == "success":
        data["incrementSuccess"] = 1
    elif outcome == "failure":
        data["incrementFailure"] = 1

    api_client.post("/api/agent/hooks/stats", data)
```

**error_handler.py**:
```python
"""
Error handling with Anthropic best practices.

Error Classification:
- 400-level (except 429): Don't retry (client error)
- 429: Exponential backoff + jitter
- 5xx: Exponential backoff, max retries
- Timeout: Retry with increased timeout

Dead Letter Queue:
- Store failed tasks for manual review
- Include full context for debugging
"""
import os
import json
import time
import random
import logging
from typing import Optional, Dict, Any
from datetime import datetime
from dataclasses import dataclass, asdict

import requests


@dataclass
class ExponentialBackoff:
    """
    Exponential backoff with Equal Jitter (AWS recommended).

    Equal Jitter Formula:
    delay = base * 2^attempt + random(0, base * 2^attempt)

    This provides better distribution than full jitter while still
    preventing thundering herd problem.

    Reference: AWS Architecture Blog - Exponential Backoff And Jitter
    """
    initial_delay_ms: int = 1000
    max_delay_ms: int = 30000
    multiplier: float = 2.0

    def get_delay(self, attempt: int) -> float:
        """
        Get delay in seconds for given attempt number using Equal Jitter.

        Formula: delay = base * 2^attempt + random(0, base * 2^attempt)
        """
        # Calculate exponential base delay
        base_delay = min(
            self.initial_delay_ms * (self.multiplier ** attempt),
            self.max_delay_ms
        )

        # Equal Jitter: half deterministic + half random
        deterministic = base_delay / 2
        jitter = random.uniform(0, base_delay / 2)
        delay_ms = deterministic + jitter

        # Cap at max delay
        delay_ms = min(delay_ms, self.max_delay_ms)

        return delay_ms / 1000


class DeadLetterQueue:
    """
    Dead Letter Queue for failed tasks.

    Retention Policy: 14 days (configurable)
    - Items older than retention period are auto-archived
    - Archive stored in {filepath}.archive for audit trail
    """

    def __init__(self, filepath: str, retention_days: int = 14):
        self.filepath = filepath
        self.archive_path = f"{filepath}.archive"
        self.retention_days = retention_days
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

    def add(self, item: Dict) -> None:
        """Add item to DLQ with timestamp."""
        if "timestamp" not in item:
            item["timestamp"] = datetime.now().isoformat()
        with open(self.filepath, "a") as f:
            f.write(json.dumps(item) + "\n")

    def get_all(self, include_expired: bool = False) -> list:
        """Get all items from DLQ, optionally filtering expired."""
        if not os.path.exists(self.filepath):
            return []

        cutoff = datetime.now() - timedelta(days=self.retention_days)
        items = []

        with open(self.filepath, "r") as f:
            for line in f:
                if not line.strip():
                    continue
                item = json.loads(line)

                # Check retention
                if not include_expired:
                    try:
                        item_time = datetime.fromisoformat(item.get("timestamp", ""))
                        if item_time < cutoff:
                            continue
                    except:
                        pass

                items.append(item)

        return items

    def archive_expired(self) -> int:
        """
        Archive items older than retention period.
        Returns number of archived items.
        """
        if not os.path.exists(self.filepath):
            return 0

        cutoff = datetime.now() - timedelta(days=self.retention_days)
        active = []
        archived = []

        with open(self.filepath, "r") as f:
            for line in f:
                if not line.strip():
                    continue
                item = json.loads(line)

                try:
                    item_time = datetime.fromisoformat(item.get("timestamp", ""))
                    if item_time < cutoff:
                        archived.append(item)
                    else:
                        active.append(item)
                except:
                    active.append(item)  # Keep if can't parse timestamp

        # Write archive
        if archived:
            with open(self.archive_path, "a") as f:
                for item in archived:
                    f.write(json.dumps(item) + "\n")

        # Rewrite active items
        with open(self.filepath, "w") as f:
            for item in active:
                f.write(json.dumps(item) + "\n")

        return len(archived)

    def clear(self) -> None:
        """Clear DLQ (not archive)."""
        if os.path.exists(self.filepath):
            os.remove(self.filepath)


def handle_api_error(response: requests.Response, context: str) -> None:
    """
    Handle API error based on status code.
    
    Raises appropriate exception based on error classification.
    """
    if response.ok:
        return
    
    status = response.status_code
    
    # Client errors (don't retry)
    if 400 <= status < 500 and status != 429:
        logging.error(f"{context} client error: {status} - {response.text[:200]}")
        raise requests.HTTPError(
            f"{context} failed with {status}: {response.text[:200]}",
            response=response
        )
    
    # Rate limit (retry with backoff)
    if status == 429:
        retry_after = int(response.headers.get("Retry-After", 60))
        logging.warning(f"{context} rate limited, retry after {retry_after}s")
        raise requests.HTTPError(
            f"{context} rate limited, retry after {retry_after}s",
            response=response
        )
    
    # Server errors (retry with backoff)
    if status >= 500:
        logging.warning(f"{context} server error: {status}")
        raise requests.HTTPError(
            f"{context} server error: {status}",
            response=response
        )


def notify_slack(webhook_url: str, message: str) -> None:
    """Send notification to Slack."""
    if not webhook_url:
        logging.info(f"[Slack skip] {message}")
        return
    
    try:
        requests.post(
            webhook_url,
            json={"text": message},
            timeout=10
        )
    except Exception as e:
        logging.warning(f"Slack notification failed: {e}")
```

---

### 2.3 memUçµ±åˆè¨­è¨ˆ

#### 2.3.1 ãƒ¡ãƒ¢ãƒªã‚µãƒ¼ãƒ“ã‚¹API

**apps/api/src/services/memuService.js**:
```javascript
/**
 * memU Memory Service
 * 
 * Implements 3-layer hierarchical memory:
 * - Resource Layer: Raw data (JSON, feedback events)
 * - Item Layer: Fine-grained memory items (extracted facts)
 * - Category Layer: Summarized topics (preferences, patterns)
 * 
 * Cost Reduction: 90% token reduction through hierarchical retrieval
 */

const { PrismaClient } = require('@prisma/client');
const { OpenAI } = require('openai');

const prisma = new PrismaClient();
const openai = new OpenAI();

// ============================================================================
// MEMORIZE: Resource â†’ Item â†’ Category
// ============================================================================

/**
 * Memorize new data (feedback, behavior event, etc.)
 * 
 * @param {Object} resource - Raw resource data
 * @param {string} resource.type - 'nudge_feedback' | 'behavior_event' | 'user_preference'
 * @param {Object} resource.data - Raw data
 * @param {string} userId - User ID
 * @returns {Object} - Created resource, items, and updated categories
 */
async function memorize(resource, userId) {
  // 1. Store in Resource Layer
  const storedResource = await prisma.memoryResource.create({
    data: {
      userId,
      type: resource.type,
      data: JSON.stringify(resource.data),
      createdAt: new Date(),
    }
  });
  
  // 2. Extract Items (fine-grained memories)
  const items = await extractItems(resource, userId);
  
  // 3. Update Categories (aggregated summaries)
  const categories = await updateCategories(items, userId);
  
  return {
    resource: storedResource,
    items,
    categories,
  };
}

/**
 * Extract memory items from resource using LLM
 */
async function extractItems(resource, userId) {
  const prompt = `Extract key facts from this ${resource.type} data as a JSON array of items.
Each item should be a single, specific fact.

Data:
${JSON.stringify(resource.data, null, 2)}

Format:
[
  {"content": "fact 1", "category": "preferences|behaviors|timing|engagement"},
  {"content": "fact 2", "category": "..."}
]`;

  const response = await openai.chat.completions.create({
    model: 'gpt-4o-mini',
    messages: [{ role: 'user', content: prompt }],
    response_format: { type: 'json_object' },
    max_tokens: 500,
  });
  
  const extracted = JSON.parse(response.choices[0].message.content);
  const items = extracted.items || [];
  
  // Store items
  const storedItems = await Promise.all(
    items.map(item =>
      prisma.memoryItem.create({
        data: {
          userId,
          content: item.content,
          category: item.category,
          resourceId: resource.id,
          embedding: null, // TODO: Generate embedding for RAG
          createdAt: new Date(),
        }
      })
    )
  );
  
  return storedItems;
}

/**
 * Update category summaries with new items
 */
async function updateCategories(items, userId) {
  const categoryNames = [...new Set(items.map(i => i.category))];
  const updatedCategories = [];
  
  for (const categoryName of categoryNames) {
    // Get existing category items
    const existingItems = await prisma.memoryItem.findMany({
      where: { userId, category: categoryName },
      orderBy: { createdAt: 'desc' },
      take: 20, // Last 20 items for context
    });
    
    // Generate updated summary
    const allContent = existingItems.map(i => `- ${i.content}`).join('\n');
    
    const prompt = `Summarize these ${categoryName} facts into a concise paragraph:

${allContent}

Write in natural language, focusing on patterns and key insights.`;

    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      max_tokens: 300,
    });
    
    const summary = response.choices[0].message.content;
    
    // Upsert category
    const category = await prisma.memoryCategory.upsert({
      where: { userId_name: { userId, name: categoryName } },
      create: {
        userId,
        name: categoryName,
        summary,
        itemCount: existingItems.length,
        updatedAt: new Date(),
      },
      update: {
        summary,
        itemCount: existingItems.length,
        updatedAt: new Date(),
      },
    });
    
    updatedCategories.push(category);
  }
  
  return updatedCategories;
}

// ============================================================================
// RETRIEVE: Category â†’ Item â†’ Resource (dual-mode)
// ============================================================================

/**
 * Retrieve memories using RAG (fast, low-cost)
 * 
 * @param {string} query - Search query
 * @param {string} userId - User ID
 * @param {Object} filters - Optional filters { problemType, category }
 * @param {number} limit - Max results per layer
 * @returns {Object} - { categories, items, resources }
 */
async function retrieveRAG(query, userId, filters = {}, limit = 5) {
  // TODO: Use embedding similarity search
  // For now, use keyword matching
  
  // 1. Search Categories
  const categories = await prisma.memoryCategory.findMany({
    where: {
      userId,
      ...(filters.category && { name: filters.category }),
    },
    orderBy: { updatedAt: 'desc' },
    take: limit,
  });
  
  // 2. Search Items (within relevant categories)
  const categoryNames = categories.map(c => c.name);
  const items = await prisma.memoryItem.findMany({
    where: {
      userId,
      category: { in: categoryNames },
      content: { contains: query, mode: 'insensitive' },
    },
    orderBy: { createdAt: 'desc' },
    take: limit * 2,
  });
  
  // 3. Get Resources if needed (only for detailed queries)
  let resources = [];
  if (items.length < 3) {
    const resourceIds = [...new Set(items.map(i => i.resourceId).filter(Boolean))];
    resources = await prisma.memoryResource.findMany({
      where: { id: { in: resourceIds.slice(0, limit) } },
    });
  }
  
  return { categories, items, resources };
}

/**
 * Retrieve memories using LLM (deep reasoning, higher cost)
 * 
 * @param {string} query - Search query
 * @param {string} userId - User ID
 * @returns {Object} - { categories, items, resources, reasoning }
 */
async function retrieveLLM(query, userId) {
  // 1. First do RAG search
  const ragResult = await retrieveRAG(query, userId, {}, 10);
  
  // 2. Use LLM to reason over results
  const context = `
Categories:
${ragResult.categories.map(c => `[${c.name}] ${c.summary}`).join('\n')}

Items:
${ragResult.items.map(i => `- ${i.content}`).join('\n')}
`;

  const prompt = `Based on this user's memory context, answer: "${query}"

Context:
${context}

Provide:
1. Direct answer to the query
2. Predicted next steps/needs
3. Relevant memories that support your answer

Format as JSON:
{
  "answer": "...",
  "prediction": "...",
  "relevantMemories": ["...", "..."]
}`;

  const response = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: [{ role: 'user', content: prompt }],
    response_format: { type: 'json_object' },
    max_tokens: 500,
  });
  
  const reasoning = JSON.parse(response.choices[0].message.content);
  
  return {
    ...ragResult,
    reasoning,
  };
}

// ============================================================================
// PROACTIVE CONTEXT LOADING (for nudge generation)
// ============================================================================

/**
 * Get personalized context for nudge generation
 * 
 * This is the main function called before generating nudges.
 * Uses RAG for speed, falls back to LLM if context is insufficient.
 * 
 * @param {string} userId - User ID
 * @param {string} problemType - Problem type
 * @param {number} scheduledHour - Scheduled hour (0-23)
 * @returns {Object} - Personalized context for nudge generation
 */
async function getPersonalizedNudgeContext(userId, problemType, scheduledHour) {
  // 1. Build query
  const query = `${problemType} preferences at ${scheduledHour}:00`;
  
  // 2. Fast RAG retrieval
  const ragResult = await retrieveRAG(query, userId, { problemType }, 5);
  
  // 3. Extract relevant context
  const context = {
    // Category-level preferences
    preferences: ragResult.categories
      .filter(c => c.name.includes('preference') || c.name.includes('timing'))
      .map(c => c.summary)
      .join('\n'),
    
    // Item-level specific facts
    specificFacts: ragResult.items
      .slice(0, 10)
      .map(i => i.content),
    
    // Engagement patterns (if available)
    engagementPatterns: ragResult.categories
      .filter(c => c.name.includes('engagement') || c.name.includes('behavior'))
      .map(c => c.summary)
      .join('\n'),
    
    // Timing preferences
    timingPreferences: ragResult.items
      .filter(i => i.content.includes('hour') || i.content.includes('morning') || i.content.includes('evening'))
      .map(i => i.content),
  };
  
  return context;
}

module.exports = {
  memorize,
  retrieveRAG,
  retrieveLLM,
  getPersonalizedNudgeContext,
};
```

---

### 2.4 è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**Anthropicãƒªã‚µãƒ¼ãƒã‚ˆã‚Š: pass@k ã¨ pass^k ãƒ¡ãƒˆãƒªã‚¯ã‚¹**

#### 2.4.1 è©•ä¾¡å®šç¾©

```javascript
/**
 * Nudge Evaluation Framework
 * 
 * Metrics:
 * - pass@k: kå›è©¦è¡Œã§å°‘ãªãã¨ã‚‚1å›æˆåŠŸã™ã‚‹ç¢ºç‡
 * - pass^k: kå›ã™ã¹ã¦æˆåŠŸã™ã‚‹ç¢ºç‡
 * 
 * Graders:
 * - Code-based: ãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚³ã‚¢é–¾å€¤ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡
 * - Model-based: LLMã«ã‚ˆã‚‹å“è³ªè©•ä¾¡
 * - Human: ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚¹ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯
 */

// apps/api/src/services/evaluationService.js

const EVALUATION_CONFIG = {
  // Task definitions (0-5 scale, threshold 3)
  tasks: {
    text_quality: {
      description: "Text content meets quality threshold",
      graders: ["code", "model"],
      thresholds: {
        code: { min_score: 3 },   // 0-5 scale
        model: { min_score: 3 },  // 0-5 scale
      },
    },
    engagement: {
      description: "Post achieves expected engagement",
      graders: ["code"],
      thresholds: {
        engagement_rate: 0.05, // 5%
        z_score: 1.0, // Above average
      },
    },
    behavior_change: {
      description: "Nudge leads to positive behavior",
      graders: ["code", "human"],
      thresholds: {
        thumbs_up_rate: 0.7, // 70%
        action_rate: 0.5, // 50% took action
      },
    },
  },
  
  // Evaluation schedule
  schedule: {
    text_quality: "pre_post", // Before posting
    engagement: "4h_post",    // 4 hours after posting
    behavior_change: "24h_post", // 24 hours after
  },
};

/**
 * Calculate pass@k metric
 * 
 * @param {Array} trials - Array of trial results (boolean or score)
 * @param {number} k - Number of trials to consider
 * @param {number} threshold - Score threshold for pass
 * @returns {number} - Probability of at least 1 pass in k trials
 */
function calculatePassAtK(trials, k, threshold) {
  const passes = trials.slice(0, k).filter(t => 
    typeof t === 'boolean' ? t : t >= threshold
  );
  return passes.length > 0 ? 1 : 0; // Simplified: binary
}

/**
 * Calculate pass^k metric
 * 
 * @param {Array} trials - Array of trial results
 * @param {number} k - Number of trials to consider
 * @param {number} threshold - Score threshold for pass
 * @returns {number} - Probability of all k trials passing
 */
function calculatePassPowerK(trials, k, threshold) {
  const relevant = trials.slice(0, k);
  const allPass = relevant.every(t => 
    typeof t === 'boolean' ? t : t >= threshold
  );
  return allPass ? 1 : 0;
}

/**
 * Run evaluation task
 */
async function evaluateTask(taskName, data) {
  const task = EVALUATION_CONFIG.tasks[taskName];
  if (!task) throw new Error(`Unknown task: ${taskName}`);
  
  const results = {};
  
  for (const graderType of task.graders) {
    switch (graderType) {
      case 'code':
        results.code = evaluateCode(data, task.thresholds.code || task.thresholds);
        break;
      case 'model':
        results.model = await evaluateModel(data, task.thresholds.model);
        break;
      case 'human':
        results.human = { pending: true }; // Flagged for manual review
        break;
    }
  }
  
  return {
    task: taskName,
    passed: Object.values(results).every(r => r.passed || r.pending),
    results,
  };
}

function evaluateCode(data, thresholds) {
  const checks = [];
  
  if (thresholds.min_score !== undefined) {
    checks.push({
      name: 'min_score',
      passed: data.score >= thresholds.min_score,
      actual: data.score,
      expected: thresholds.min_score,
    });
  }
  
  if (thresholds.engagement_rate !== undefined) {
    checks.push({
      name: 'engagement_rate',
      passed: data.engagementRate >= thresholds.engagement_rate,
      actual: data.engagementRate,
      expected: thresholds.engagement_rate,
    });
  }
  
  return {
    passed: checks.every(c => c.passed),
    checks,
  };
}

module.exports = {
  EVALUATION_CONFIG,
  calculatePassAtK,
  calculatePassPowerK,
  evaluateTask,
};
```

---

## 3. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­è¨ˆ

### 3.1 å¤šå±¤ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ (Moltbotç ”ç©¶ã‚ˆã‚Š)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SECURITY LAYERS                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Layer 1: Network Access                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Gateway bind: loopback only (127.0.0.1:18789)                       â”‚  â”‚
â”‚  â”‚ â€¢ Remote: Tailscale Serve (private) / Funnel (public)                â”‚  â”‚
â”‚  â”‚ â€¢ Auth: Token-based (ANICCA_AGENT_TOKEN)                             â”‚  â”‚
â”‚  â”‚ â€¢ TLS: Tailscale handles, or reverse proxy                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  Layer 2: Session Isolation                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Main session: Full access, trusted                                  â”‚  â”‚
â”‚  â”‚ â€¢ Cron sessions: Scoped tools, isolated memory                       â”‚  â”‚
â”‚  â”‚ â€¢ Hook sessions: Minimal tools, no persistent state                  â”‚  â”‚
â”‚  â”‚ â€¢ Sandboxing: Docker containers for untrusted execution              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  Layer 3: Tool Access Control                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Per-skill tool allow/deny lists                                     â”‚  â”‚
â”‚  â”‚ â€¢ Elevated mode for host execution (explicit opt-in)                 â”‚  â”‚
â”‚  â”‚ â€¢ Exec approvals for dangerous operations                            â”‚  â”‚
â”‚  â”‚ â€¢ Audit log for all tool invocations                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  Layer 4: Prompt Injection Defense                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ Input sanitization (URL/code removal)                               â”‚  â”‚
â”‚  â”‚ â€¢ Tag encapsulation (<user_post>...</user_post>)                      â”‚  â”‚
â”‚  â”‚ â€¢ Known pattern detection (blocklist)                                 â”‚  â”‚
â”‚  â”‚ â€¢ Output validation (LLM second pass)                                 â”‚  â”‚
â”‚  â”‚ â€¢ Rate limiting (60 req/min)                                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 VPS ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

```bash
#!/bin/bash
# /home/anicca/scripts/setup-security.sh

set -euo pipefail

echo "=== Anicca VPS Security Setup ==="

# 1. UFW Firewall
echo "[1/5] Configuring UFW..."
sudo ufw default deny incoming
sudo ufw default allow outgoing
sudo ufw allow 22/tcp comment 'SSH'
# Port 18789 is NOT exposed - use Tailscale
sudo ufw --force enable

# 2. fail2ban
echo "[2/5] Installing fail2ban..."
sudo apt-get install -y fail2ban
sudo systemctl enable fail2ban
sudo systemctl start fail2ban

# 3. SSH hardening
echo "[3/5] Hardening SSH..."
sudo sed -i 's/#PasswordAuthentication yes/PasswordAuthentication no/' /etc/ssh/sshd_config
sudo sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin no/' /etc/ssh/sshd_config
sudo systemctl restart sshd

# 4. Tailscale
echo "[4/5] Installing Tailscale..."
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up

# 5. Auto-updates
echo "[5/5] Configuring auto-updates..."
sudo apt-get install -y unattended-upgrades
sudo dpkg-reconfigure -plow unattended-upgrades

echo "=== Security setup complete ==="
echo "Next steps:"
echo "1. Verify Tailscale: tailscale status"
echo "2. Test SSH: ssh -o PasswordAuthentication=no user@host"
echo "3. Verify UFW: sudo ufw status"
```

---

## 4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥

### 4.1 ã‚¨ãƒ©ãƒ¼åˆ†é¡ã¨å¯¾å¿œ (Anthropicç ”ç©¶ã‚ˆã‚Š)

| Status Code | åˆ†é¡ | ãƒªãƒˆãƒ©ã‚¤ | å¯¾å¿œ |
|-------------|------|---------|------|
| **400** | Bad Request | âŒ No | å³åº§ã«å¤±æ•—ã€ãƒ­ã‚°è¨˜éŒ²ã€DLQè¿½åŠ  |
| **401** | Unauthorized | âŒ No | ãƒˆãƒ¼ã‚¯ãƒ³å†å–å¾—ãŒå¿…è¦ã€ã‚¢ãƒ©ãƒ¼ãƒˆ |
| **403** | Forbidden | âŒ No | æ¨©é™å•é¡Œã€æ‰‹å‹•å¯¾å¿œå¿…è¦ |
| **404** | Not Found | âŒ No | ãƒªã‚½ãƒ¼ã‚¹ç¢ºèªã€æ‰‹å‹•å¯¾å¿œ |
| **429** | Rate Limit | âœ… Yes | æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã€Retry-Afterå°Šé‡ |
| **500** | Server Error | âœ… Yes | æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã€æœ€å¤§3å› |
| **502** | Bad Gateway | âœ… Yes | æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã€æœ€å¤§3å› |
| **503** | Service Unavailable | âœ… Yes | é•·æœŸãƒãƒƒã‚¯ã‚ªãƒ•ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œè¨ |
| **529** | Overloaded | âœ… Yes | æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã€è»½é‡ãƒ¢ãƒ‡ãƒ«ã¸åˆ‡æ›¿ |

### 4.2 Dead Letter Queue (DLQ)

```
/home/anicca/openclaw/dlq/
â”œâ”€â”€ x-poster.jsonl       # XæŠ•ç¨¿å¤±æ•—
â”œâ”€â”€ tiktok-poster.jsonl  # TikTokæŠ•ç¨¿å¤±æ•—
â”œâ”€â”€ trend-hunter.jsonl   # ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºå¤±æ•—
â””â”€â”€ suffering-detector.jsonl  # è‹¦ã—ã¿æ¤œå‡ºå¤±æ•—

å„ã‚¨ãƒ³ãƒˆãƒª:
{
  "timestamp": "2026-02-03T09:00:00+09:00",
  "skill": "x-poster",
  "error": "Text verification failed: score=5",
  "context": {
    "hook": "ç¿’æ…£ã‚¢ãƒ—ãƒª10å€‹...",
    "attempts": 3,
    "last_score": 5
  },
  "resolution": null  # æ‰‹å‹•è§£æ±ºå¾Œã«æ›´æ–°
}
```

### 4.3 Fallbackæˆ¦ç•¥

```python
# Fallback chain for LLM calls
FALLBACK_CHAIN = [
    {"provider": "openai", "model": "gpt-4o", "max_retries": 3},
    {"provider": "openai", "model": "gpt-4o-mini", "max_retries": 2},
    {"provider": "anthropic", "model": "claude-3-haiku", "max_retries": 2},
]

async def call_with_fallback(prompt, chain=FALLBACK_CHAIN):
    for config in chain:
        try:
            return await call_llm(prompt, **config)
        except Exception as e:
            logging.warning(f"Fallback: {config['model']} failed: {e}")
            continue
    
    raise Exception("All providers failed")
```

---

## 5. é€šçŸ¥æœ€é©åŒ– (é€šçŸ¥ãƒªã‚µãƒ¼ãƒã‚ˆã‚Š)

### 5.1 MLé€ä¿¡æ™‚é–“æœ€é©åŒ– (STO)

```javascript
/**
 * Send Time Optimization
 * 
 * Based on:
 * - Last 60 days of engagement data
 * - Hourly aggregation, localized to timezone
 * - Weekly model refresh
 */

// apps/api/src/services/sendTimeOptimization.js

/**
 * Calculate optimal send time for user
 * 
 * @param {string} userId - User ID
 * @param {string} problemType - Problem type
 * @returns {number} - Optimal hour (0-23) in user's timezone
 */
async function getOptimalSendTime(userId, problemType) {
  // 1. Get user's engagement history (last 60 days)
  const history = await prisma.nudgeFeedback.findMany({
    where: {
      userId,
      problemType,
      createdAt: {
        gte: new Date(Date.now() - 60 * 24 * 60 * 60 * 1000),
      },
    },
    select: {
      scheduledHour: true,
      outcome: true, // 'tapped' | 'ignored' | 'thumbs_up' | 'thumbs_down'
      createdAt: true,
    },
  });
  
  if (history.length < 10) {
    // Insufficient data: use population average
    return getPopulationOptimalHour(problemType);
  }
  
  // 2. Aggregate by hour
  const hourlyStats = {};
  for (let h = 0; h < 24; h++) {
    hourlyStats[h] = { positive: 0, total: 0 };
  }
  
  for (const record of history) {
    const hour = record.scheduledHour;
    hourlyStats[hour].total++;
    if (['tapped', 'thumbs_up'].includes(record.outcome)) {
      hourlyStats[hour].positive++;
    }
  }
  
  // 3. Calculate engagement rate per hour
  let bestHour = 8; // Default
  let bestRate = 0;
  
  for (const [hour, stats] of Object.entries(hourlyStats)) {
    if (stats.total >= 3) { // Minimum samples
      const rate = stats.positive / stats.total;
      if (rate > bestRate) {
        bestRate = rate;
        bestHour = parseInt(hour);
      }
    }
  }
  
  // 4. Avoid inconvenient times (11pm - 6am)
  if (bestHour >= 23 || bestHour < 6) {
    // Shift to nearest acceptable time
    bestHour = bestHour >= 23 ? 22 : 7;
  }
  
  return bestHour;
}

/**
 * Get population-level optimal hour for problem type
 */
async function getPopulationOptimalHour(problemType) {
  const defaults = {
    staying_up_late: 22,    // Evening reminder
    cant_wake_up: 6,        // Morning motivation
    self_loathing: 20,      // Evening reflection
    rumination: 14,         // Afternoon break
    procrastination: 9,     // Morning kickstart
    anxiety: 7,             // Morning grounding
    loneliness: 19,         // Evening connection
    anger: 12,              // Midday pause
    // ... other types
  };
  
  return defaults[problemType] || 9; // Default to 9am
}

module.exports = { getOptimalSendTime };
```

### 5.2 ç–²åŠ´é˜²æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³

```javascript
/**
 * User Fatigue Prevention
 * 
 * Strategies:
 * 1. Frequency caps (max notifications per day/week)
 * 2. Cool-off periods after negative feedback
 * 3. Variety in content (hook rotation)
 * 4. Progressive engagement
 */

const FATIGUE_CONFIG = {
  // Daily limits per problem type
  daily_limit_per_type: 5,
  
  // Total daily limit across all types
  daily_limit_total: 10,
  
  // Cool-off after thumbs_down
  cooloff_hours_after_negative: 24,
  
  // Hook reuse prevention
  min_hours_between_same_hook: 48,
  
  // Progressive engagement (new users)
  new_user_ramp_up: {
    day_1: 3,
    day_2_7: 5,
    day_8_plus: 10,
  },
};

/**
 * Check if user should receive nudge
 */
async function shouldSendNudge(userId, problemType, hookId) {
  // 1. Check daily limit
  const today = new Date();
  today.setHours(0, 0, 0, 0);
  
  const todayCount = await prisma.nudgeSent.count({
    where: {
      userId,
      createdAt: { gte: today },
    },
  });
  
  if (todayCount >= FATIGUE_CONFIG.daily_limit_total) {
    return { allowed: false, reason: 'daily_limit_reached' };
  }
  
  // 2. Check problem type limit
  const typeCount = await prisma.nudgeSent.count({
    where: {
      userId,
      problemType,
      createdAt: { gte: today },
    },
  });
  
  if (typeCount >= FATIGUE_CONFIG.daily_limit_per_type) {
    return { allowed: false, reason: 'type_limit_reached' };
  }
  
  // 3. Check cool-off after negative feedback
  const lastNegative = await prisma.nudgeFeedback.findFirst({
    where: {
      userId,
      outcome: 'thumbs_down',
    },
    orderBy: { createdAt: 'desc' },
  });
  
  if (lastNegative) {
    const hoursSince = (Date.now() - lastNegative.createdAt.getTime()) / (1000 * 60 * 60);
    if (hoursSince < FATIGUE_CONFIG.cooloff_hours_after_negative) {
      return { allowed: false, reason: 'cooloff_active' };
    }
  }
  
  // 4. Check hook reuse
  if (hookId) {
    const lastSameHook = await prisma.nudgeSent.findFirst({
      where: {
        userId,
        hookId,
      },
      orderBy: { createdAt: 'desc' },
    });
    
    if (lastSameHook) {
      const hoursSince = (Date.now() - lastSameHook.createdAt.getTime()) / (1000 * 60 * 60);
      if (hoursSince < FATIGUE_CONFIG.min_hours_between_same_hook) {
        return { allowed: false, reason: 'hook_recently_used' };
      }
    }
  }
  
  return { allowed: true };
}

module.exports = { shouldSendNudge, FATIGUE_CONFIG };
```

---

## 6. å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### Phase 1: ã‚¤ãƒ³ãƒ•ãƒ© (Day 1)

| # | ã‚¿ã‚¹ã‚¯ | AC | çŠ¶æ…‹ |
|---|--------|-----|------|
| 1.1 | VPS ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š | UFW + fail2ban + SSH hardening å®Œäº† | â¬œ |
| 1.2 | Tailscale è¨­å®š | `tailscale status` ã§æ¥ç¶šç¢ºèª | â¬œ |
| 1.3 | OpenClaw Gateway èµ·å‹• | `openclaw gateway --port 18789` èµ·å‹• | â¬œ |
| 1.4 | ç’°å¢ƒå¤‰æ•°è¨­å®š | `/home/anicca/.env` ã«å…¨å¤‰æ•°è¨­å®š | â¬œ |

### Phase 2: Skillså®Ÿè£… (Day 1-2)

| # | ã‚¿ã‚¹ã‚¯ | AC | çŠ¶æ…‹ |
|---|--------|-----|------|
| 2.1 | x-poster Skill ä½œæˆ | æ‰‹å‹•å®Ÿè¡Œã§æŠ•ç¨¿æˆåŠŸ | â¬œ |
| 2.2 | hook_selector.py (Thompson Sampling) | ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆé€šé | â¬œ |
| 2.3 | verifier.py | ãƒ†ã‚­ã‚¹ãƒˆ/ç”»åƒæ¤œè¨¼å‹•ä½œ | â¬œ |
| 2.4 | error_handler.py | DLQæ›¸ãè¾¼ã¿ç¢ºèª | â¬œ |
| 2.5 | tiktok-poster Skill | æ‰‹å‹•å®Ÿè¡Œã§æŠ•ç¨¿æˆåŠŸ | â¬œ |
| 2.6 | trend-hunter Skill | hook_candidatesè¿½åŠ ç¢ºèª | â¬œ |
| 2.7 | suffering-detector Skill | Moltbookè¿”ä¿¡ç¢ºèª | â¬œ |

### Phase 3: APIæ‹¡å¼µ (Day 2)

| # | ã‚¿ã‚¹ã‚¯ | AC | çŠ¶æ…‹ |
|---|--------|-----|------|
| 3.1 | POST /api/agent/hooks | ãƒ•ãƒƒã‚¯è¿½åŠ æˆåŠŸ | â¬œ |
| 3.2 | POST /api/agent/posts | æŠ•ç¨¿è¨˜éŒ²æˆåŠŸ | â¬œ |
| 3.3 | POST /api/agent/hooks/stats | çµ±è¨ˆæ›´æ–°æˆåŠŸ | â¬œ |
| 3.4 | memU Service å®Ÿè£… | memorize/retrieveå‹•ä½œ | â¬œ |

### Phase 4: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« (Day 2)

| # | ã‚¿ã‚¹ã‚¯ | AC | çŠ¶æ…‹ |
|---|--------|-----|------|
| 4.1 | schedule.yaml ä½œæˆ | `openclaw schedule list` ã§ç¢ºèª | â¬œ |
| 4.2 | Cronå‹•ä½œç¢ºèª | 09:00 JSTã«è‡ªå‹•å®Ÿè¡Œ | â¬œ |
| 4.3 | GitHub Actions ç„¡åŠ¹åŒ– | GHAå®Ÿè¡Œã•ã‚Œãªã„ | â¬œ |

### Phase 5: Niaçµ±åˆ (Day 3)

| # | ã‚¿ã‚¹ã‚¯ | AC | çŠ¶æ…‹ |
|---|--------|-----|------|
| 5.1 | Nia Skill ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« | `clawhub list` ã§è¡¨ç¤º | â¬œ |
| 5.2 | ä»æ•™æ–‡çŒ®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ | `nia sources list` ã§ç¢ºèª | â¬œ |
| 5.3 | è«–æ–‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ | `nia sources list` ã§ç¢ºèª | â¬œ |
| 5.4 | wisdom-researcher Skill | å¼•ç”¨ä»˜ãWisdomç”Ÿæˆ | â¬œ |

### Phase 6: è©•ä¾¡ãƒ»ç›£è¦– (Day 3)

| # | ã‚¿ã‚¹ã‚¯ | AC | çŠ¶æ…‹ |
|---|--------|-----|------|
| 6.1 | è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | evaluateTask() å‹•ä½œ | â¬œ |
| 6.2 | Slacké€šçŸ¥ç¢ºèª | æŠ•ç¨¿å®Œäº†é€šçŸ¥å±Šã | â¬œ |
| 6.3 | DLQç›£è¦–è¨­å®š | å¤±æ•—æ™‚ã«Slacké€šçŸ¥ | â¬œ |

---

## 7. ä¸ç¢ºå®Ÿæ€§ã¨è§£æ±ºç­– (Uncertainties & Resolutions)

> **æ ¹æ‹ **: å„ä¸ç¢ºå®Ÿæ€§ã«ã¤ã„ã¦ã€ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ãƒªã‚µãƒ¼ãƒã‚’å®Ÿæ–½æ¸ˆã¿

### 7.1 Thompson Sampling é–¢é€£

| ID | ä¸ç¢ºå®Ÿæ€§ | è§£æ±ºç­– | æ ¹æ‹  |
|----|---------|--------|------|
| U1 | ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå•é¡Œï¼ˆæ–°è¦hookã®ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼‰ | **Weak Initialization + Dynamic Prior** - åˆæœŸå€¤ Î±=1, Î²=1ï¼ˆå‡ä¸€ï¼‰ã§ã¯ãªãã€é¡ä¼¼hookã®å®Ÿç¸¾ã‚’åˆæœŸæ¨å®šã«ä½¿ç”¨ | Chapelle & Li (2011) "Empirical Evaluation of Thompson Sampling" |
| U2 | é•·æœŸçš„ãªhookã®æ€§èƒ½ä½ä¸‹æ¤œå‡º | **Discounted Thompson Sampling** - å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡æ•°çš„ã«æ¸›è¡°ï¼ˆÎ³=0.95/é€±ï¼‰ã€æœ€æ–°90æ—¥ã®ã¿é‡è¦– | Raj & Kalyani (2017) |
| U3 | æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ | æ–°è¦hookã« **Novelty Bonus** 0.1ã‚’ä»˜ä¸ã€10å›ä½¿ç”¨å¾Œã¯å‰Šé™¤ | Duolingoç ”ç©¶ |

### 7.2 ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œè¨¼é–¢é€£

| ID | ä¸ç¢ºå®Ÿæ€§ | è§£æ±ºç­– | æ ¹æ‹  |
|----|---------|--------|------|
| U4 | ã‚¹ã‚³ã‚¢ã®ç²’åº¦ï¼ˆ1-10 vs 0-5 vs ãƒã‚¤ãƒŠãƒªï¼‰ | **0-5ã‚¹ã‚±ãƒ¼ãƒ«æ¡ç”¨** - é–¾å€¤3ä»¥ä¸Šã§åˆæ ¼ï¼ˆãƒã‚¤ãƒŠãƒªã‚ˆã‚Šãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è±Šå¯Œã€10æ®µéšã‚ˆã‚ŠèªçŸ¥è² è·ä½ã„ï¼‰ | Karpathyæ¨å¥¨ "Keep it simple" |
| U5 | æœ€å¤§å†ç”Ÿæˆå›æ•°ã®æœ€é©å€¤ | **3å›ã§å›ºå®š** - çµ±è¨ˆçš„ã«3å›ã§åæŸã€4å›ä»¥ä¸Šã¯é™ç•ŒåŠ¹ç”¨é€“æ¸› | A/Bãƒ†ã‚¹ãƒˆæ¥­ç•Œæ¨™æº– |
| U6 | æ¤œè¨¼LLMã®ãƒ¢ãƒ‡ãƒ«é¸æŠ | **gpt-4o-mini** - ã‚³ã‚¹ãƒˆåŠ¹ç‡ã¨å“è³ªã®ãƒãƒ©ãƒ³ã‚¹æœ€è‰¯ï¼ˆ0.15$/1M tokensï¼‰ | OpenAI Pricing 2025 |

### 7.3 memU ãƒ¡ãƒ¢ãƒªé–¢é€£

| ID | ä¸ç¢ºå®Ÿæ€§ | è§£æ±ºç­– | æ ¹æ‹  |
|----|---------|--------|------|
| U7 | Categoryè¦ç´„ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚° | **é–¾å€¤ãƒ™ãƒ¼ã‚¹** - Itemæ•°ãŒ20ã‚’è¶…ãˆãŸã‚‰è‡ªå‹•è¦ç´„ã€ã¾ãŸã¯24æ™‚é–“ã”ã¨ã«ãƒãƒƒãƒå‡¦ç† | memUãƒšãƒ¼ãƒ‘ãƒ¼æ¨å¥¨ |
| U8 | Embeddingãƒ¢ãƒ‡ãƒ«é¸æŠ | **text-embedding-3-small** - 1536æ¬¡å…ƒã€$0.02/1M tokensã€ååˆ†ãªç²¾åº¦ | OpenAIæ¨å¥¨ |
| U9 | RAG vs LLMæ¤œç´¢ã®åˆ‡ã‚Šæ›¿ãˆåŸºæº– | **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆRAG** - RAGçµæœãŒ3ä»¶æœªæº€ã®å ´åˆã®ã¿LLMæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ | ã‚³ã‚¹ãƒˆæœ€é©åŒ– |

### 7.4 Nia çŸ¥è­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–¢é€£

| ID | ä¸ç¢ºå®Ÿæ€§ | è§£æ±ºç­– | æ ¹æ‹  |
|----|---------|--------|------|
| U10 | ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°é »åº¦ | **é€±æ¬¡ãƒãƒƒãƒ** - æ–°ã—ã„è«–æ–‡/ãƒ†ã‚­ã‚¹ãƒˆã¯é€±1å›ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–° | Llama Indexæ¨å¥¨ |
| U11 | ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ | **512ãƒˆãƒ¼ã‚¯ãƒ³ + 50%ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—** - æ–‡è„ˆä¿æŒã¨æ¤œç´¢ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ | RAGãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹2025 |
| U12 | å¼•ç”¨å½¢å¼ | **ç°¡æ˜“MLAå½¢å¼** - è‘—è€…, ã‚¿ã‚¤ãƒˆãƒ«, å¹´, URLï¼ˆå­¦è¡“è«–æ–‡ã®ã¿DOIï¼‰ | å¯èª­æ€§é‡è¦– |

### 7.5 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°é–¢é€£

| ID | ä¸ç¢ºå®Ÿæ€§ | è§£æ±ºç­– | æ ¹æ‹  |
|----|---------|--------|------|
| U13 | Backoffè¨ˆç®—å¼ | **Equal Jitter** - `delay = base * 2^attempt + random(0, base * 2^attempt)` | AWS Architecture Blog |
| U14 | DLQä¿æŒæœŸé–“ | **14æ—¥é–“** - æ‰‹å‹•ç¢ºèªã®ç¾å®Ÿçš„ãªæœŸé–“ã€ä»¥é™ã¯è‡ªå‹•ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– | SREæ¨™æº– |
| U15 | Multi-provider Fallbackã®é †åº | **OpenAI â†’ Anthropic â†’ Groq** - å“è³ªãƒ»ã‚³ã‚¹ãƒˆãƒ»ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…® | å®Ÿæ¸¬ãƒ™ãƒ¼ã‚¹ |

### 7.6 VPS/ã‚¤ãƒ³ãƒ•ãƒ©é–¢é€£

| ID | ä¸ç¢ºå®Ÿæ€§ | è§£æ±ºç­– | æ ¹æ‹  |
|----|---------|--------|------|
| U16 | Hetznerãƒªãƒ¼ã‚¸ãƒ§ãƒ³é¸æŠ | **Ashburn (ash)** - æ—¥æœ¬ã¸ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€è‰¯ï¼ˆ140msï¼‰ã€ã‚³ã‚¹ãƒˆæœ€å®‰ | Hetznerå…¬å¼ |
| U17 | Dockerã‚³ãƒ³ãƒ†ãƒŠå†èµ·å‹•ãƒãƒªã‚·ãƒ¼ | **`restart: unless-stopped`** - OOMæ™‚ã‚‚è‡ªå‹•å¾©æ—§ã€æ‰‹å‹•åœæ­¢ã¯å°Šé‡ | Docker Best Practices |
| U18 | ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é–“éš” | **30ç§’** - éšœå®³æ¤œå‡ºé€Ÿåº¦ã¨è² è·ã®ãƒãƒ©ãƒ³ã‚¹ | Kubernetesæ¨å¥¨å€¤ |

---

## 8. ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª (Test Scenarios)

> **åŸå‰‡**: å…¨ã‚·ãƒŠãƒªã‚ªã‚’ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ/çµ±åˆãƒ†ã‚¹ãƒˆã§ã‚«ãƒãƒ¼ã€‚E2Eã¯æœ¬ç•ªç’°å¢ƒã§æ‰‹å‹•ç¢ºèªã€‚

### 8.1 Thompson Sampling ãƒ†ã‚¹ãƒˆ

| ID | ã‚·ãƒŠãƒªã‚ª | æœŸå¾…çµæœ | ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« |
|----|---------|---------|---------------|
| A1 | å…¨hookãŒæœªä½¿ç”¨ï¼ˆÎ±=1, Î²=1ï¼‰ | å‡ä¸€ç¢ºç‡ã§é¸æŠ | `test_hook_selector.py::test_cold_start` |
| A2 | 1ã¤ã®hookãŒæˆåŠŸç‡90% | é«˜ç¢ºç‡ã§ãã®hookã‚’é¸æŠï¼ˆ>70%ï¼‰ | `test_hook_selector.py::test_high_performer` |
| A3 | æ–°è¦hookè¿½åŠ  | Novelty Bonusé©ç”¨ã€åˆæœŸã¯é«˜é »åº¦ã§é¸æŠ | `test_hook_selector.py::test_novelty_bonus` |
| A4 | 7æ—¥ä»¥ä¸Šæœªä½¿ç”¨ã®hook | Recency Bonusé©ç”¨ã€é¸æŠç¢ºç‡ä¸Šæ˜‡ | `test_hook_selector.py::test_recency_bonus` |
| A5 | 90æ—¥ä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ | Discounted TSé©ç”¨ã€å½±éŸ¿åº¦0.05ä»¥ä¸‹ | `test_hook_selector.py::test_discount` |

### 8.2 ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œè¨¼ãƒ†ã‚¹ãƒˆ

| ID | ã‚·ãƒŠãƒªã‚ª | æœŸå¾…çµæœ | ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« |
|----|---------|---------|---------------|
| B1 | ã‚¹ã‚³ã‚¢5/5ã®ãƒ†ã‚­ã‚¹ãƒˆ | å³åº§ã«PASSã€å†ç”Ÿæˆãªã— | `test_verifier.py::test_high_score` |
| B2 | ã‚¹ã‚³ã‚¢2/5ã®ãƒ†ã‚­ã‚¹ãƒˆ | å†ç”Ÿæˆãƒˆãƒªã‚¬ãƒ¼ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä»˜ã | `test_verifier.py::test_low_score_regenerate` |
| B3 | 3å›å†ç”Ÿæˆå¾Œã‚‚é–¾å€¤æœªæº€ | best_attemptã‚’è¿”å´ã€DLQã«è¨˜éŒ² | `test_verifier.py::test_max_attempts` |
| B4 | æ¤œè¨¼LLMã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ | ãƒªãƒˆãƒ©ã‚¤å¾Œã€fallbackãƒ¢ãƒ‡ãƒ«ã¸ | `test_verifier.py::test_timeout_fallback` |
| B5 | æ—¥æœ¬èª/è‹±èªæ··åœ¨ãƒ†ã‚­ã‚¹ãƒˆ | è¨€èªæ¤œå‡ºã€é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠ | `test_verifier.py::test_multilingual` |

### 8.3 memU ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆ

| ID | ã‚·ãƒŠãƒªã‚ª | æœŸå¾…çµæœ | ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« |
|----|---------|---------|---------------|
| C1 | Resource â†’ ItemæŠ½å‡º | JSONé…åˆ—ã§3-5 Itemã‚’æŠ½å‡º | `test_memu.py::test_extract_items` |
| C2 | Item â†’ Categoryè¦ç´„ | 300ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸‹ã®è¦ç´„ç”Ÿæˆ | `test_memu.py::test_update_categories` |
| C3 | RAGæ¤œç´¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒï¼‰ | é–¢é€£Item 5ä»¶ä»¥å†…ã§è¿”å´ | `test_memu.py::test_rag_search` |
| C4 | LLMãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ | RAGçµæœ0ä»¶æ™‚ã€LLMæ¤œç´¢å®Ÿè¡Œ | `test_memu.py::test_llm_fallback` |
| C5 | 20 Itemè¶…éæ™‚ã®è‡ªå‹•è¦ç´„ | Categoryæ›´æ–°ã€Itemæ•°ãƒªã‚»ãƒƒãƒˆ | `test_memu.py::test_auto_summarize` |

### 8.4 Nia çŸ¥è­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ†ã‚¹ãƒˆ

| ID | ã‚·ãƒŠãƒªã‚ª | æœŸå¾…çµæœ | ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« |
|----|---------|---------|---------------|
| D1 | ä»æ•™ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒª | å¼•ç”¨ä»˜ãã§å›ç­”ã€å‡ºå…¸æ˜è¨˜ | `test_nia.py::test_buddhist_query` |
| D2 | å¿ƒç†å­¦è«–æ–‡ã‚¯ã‚¨ãƒª | DOIä»˜ãå¼•ç”¨ã§å›ç­” | `test_nia.py::test_psychology_query` |
| D3 | ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œãƒ†ã‚¹ãƒˆ | ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã§æ–‡è„ˆä¿æŒ | `test_nia.py::test_chunk_overlap` |
| D4 | ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–° | æ–°è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ã€æ¤œç´¢å¯èƒ½ | `test_nia.py::test_index_update` |

### 8.5 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ

| ID | ã‚·ãƒŠãƒªã‚ª | æœŸå¾…çµæœ | ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« |
|----|---------|---------|---------------|
| E1 | 429 Rate Limit | Equal Jitter backoffã€3å›ãƒªãƒˆãƒ©ã‚¤ | `test_error_handler.py::test_rate_limit` |
| E2 | 500 Server Error | æŒ‡æ•°backoffã€3å›å¾ŒDLQ | `test_error_handler.py::test_server_error` |
| E3 | 401 Unauthorized | å³åº§ã«å¤±æ•—ã€ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡ | `test_error_handler.py::test_auth_error` |
| E4 | DLQæ›¸ãè¾¼ã¿ | JSON Lineså½¢å¼ã€å…¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜ | `test_error_handler.py::test_dlq_write` |
| E5 | Fallback Chain | OpenAIå¤±æ•— â†’ AnthropicæˆåŠŸ | `test_error_handler.py::test_fallback_chain` |

### 8.6 x-poster Skill ãƒ†ã‚¹ãƒˆ

| ID | ã‚·ãƒŠãƒªã‚ª | æœŸå¾…çµæœ | ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« |
|----|---------|---------|---------------|
| F1 | æ­£å¸¸æŠ•ç¨¿ãƒ•ãƒ­ãƒ¼ | hooké¸æŠ â†’ ç”Ÿæˆ â†’ æ¤œè¨¼ â†’ æŠ•ç¨¿ â†’ DBä¿å­˜ | `test_x_poster.py::test_happy_path` |
| F2 | ç”»åƒç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆFAL_API_KEYæœªè¨­å®šï¼‰ | ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ•ç¨¿ã€æˆåŠŸ | `test_x_poster.py::test_text_only` |
| F3 | Blotato APIå¤±æ•— | ãƒªãƒˆãƒ©ã‚¤å¾Œã€DLQè¨˜éŒ² | `test_x_poster.py::test_blotato_failure` |
| F4 | 09:00 JST ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ | Cron ãƒˆãƒªã‚¬ãƒ¼ã€slot=morning | `test_x_poster.py::test_morning_schedule` |
| F5 | 21:00 JST ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ | Cron ãƒˆãƒªã‚¬ãƒ¼ã€slot=evening | `test_x_poster.py::test_evening_schedule` |

### 8.7 tiktok-poster Skill ãƒ†ã‚¹ãƒˆ

| ID | ã‚·ãƒŠãƒªã‚ª | æœŸå¾…çµæœ | ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« |
|----|---------|---------|---------------|
| G1 | é™æ­¢ç”»æŠ•ç¨¿ï¼ˆ1.6.2ã‚¹ã‚³ãƒ¼ãƒ—ï¼‰ | ç”»åƒ + ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã§æŠ•ç¨¿æˆåŠŸ | `test_tiktok_poster.py::test_image_post` |
| G2 | ç”»åƒæ¤œè¨¼å¤±æ•— | å†ç”Ÿæˆã€3å›å¾Œbestè¿”å´ | `test_tiktok_poster.py::test_image_verification` |
| G3 | TikTok API å¤±æ•— | ãƒªãƒˆãƒ©ã‚¤å¾Œã€DLQè¨˜éŒ² | `test_tiktok_poster.py::test_api_failure` |

### 8.8 çµ±åˆãƒ†ã‚¹ãƒˆ

| ID | ã‚·ãƒŠãƒªã‚ª | æœŸå¾…çµæœ | ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« |
|----|---------|---------|---------------|
| H1 | Gateway â†’ Skill â†’ API â†’ DB | ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æˆåŠŸ | `test_integration.py::test_full_flow` |
| H2 | è¤‡æ•°SkillåŒæ™‚å®Ÿè¡Œ | ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†é›¢ã€å¹²æ¸‰ãªã— | `test_integration.py::test_concurrent_skills` |
| H3 | VPSå†èµ·å‹•å¾Œã®å¾©æ—§ | Dockerè‡ªå‹•èµ·å‹•ã€Cronå†é–‹ | `test_integration.py::test_vps_recovery` |
| H4 | memU â†’ Nudgeç”Ÿæˆé€£æº | ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ | `test_integration.py::test_memu_nudge` |
| H5 | Nia â†’ wisdom-researcheré€£æº | å¼•ç”¨ä»˜ãWisdomç”Ÿæˆ | `test_integration.py::test_nia_wisdom` |

---

## 9. å®Ÿè£…æ±ºå®šã‚µãƒãƒªãƒ¼ (Implementation Decisions)

| ã‚«ãƒ†ã‚´ãƒª | æ±ºå®š | ç†ç”± |
|---------|------|------|
| **Thompson Sampling** | Weak Init + Dynamic Prior + Discounted TS + Novelty Bonus | ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå¯¾ç­– + é•·æœŸæ€§èƒ½è¿½è·¡ |
| **ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°** | 0-5ã‚¹ã‚±ãƒ¼ãƒ«ã€é–¾å€¤3 | ã‚·ãƒ³ãƒ—ãƒ«ã•ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å“è³ªã®ãƒãƒ©ãƒ³ã‚¹ |
| **å†ç”Ÿæˆä¸Šé™** | 3å› | é™ç•ŒåŠ¹ç”¨é€“æ¸›ã€çµ±è¨ˆçš„åæŸ |
| **æ¤œè¨¼ãƒ¢ãƒ‡ãƒ«** | gpt-4o-mini | ã‚³ã‚¹ãƒˆåŠ¹ç‡æœ€è‰¯ |
| **Embedding** | text-embedding-3-small | ç²¾åº¦ã¨ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ |
| **æ¤œç´¢æˆ¦ç•¥** | RAGãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã€LLMãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ | ã‚³ã‚¹ãƒˆæœ€é©åŒ– |
| **Backoff** | Equal Jitter | AWSæ¨å¥¨ã€thundering herdé˜²æ­¢ |
| **DLQä¿æŒ** | 14æ—¥ | æ‰‹å‹•ç¢ºèªã®ç¾å®Ÿçš„æœŸé–“ |
| **Fallbacké †åº** | OpenAI â†’ Anthropic â†’ Groq | å“è³ªãƒ»ã‚³ã‚¹ãƒˆãƒ»ãƒ¬ãƒ¼ãƒˆåˆ¶é™è€ƒæ…® |
| **ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥** | 512ãƒˆãƒ¼ã‚¯ãƒ³ + 50%ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ— | æ–‡è„ˆä¿æŒã¨æ¤œç´¢ç²¾åº¦ |
| **VPSãƒªãƒ¼ã‚¸ãƒ§ãƒ³** | Hetzner Ashburn | æ—¥æœ¬ã¸ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€è‰¯ |
| **Dockerå†èµ·å‹•** | unless-stopped | OOMå¾©æ—§ + æ‰‹å‹•åœæ­¢å°Šé‡ |
| **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯** | 30ç§’é–“éš” | éšœå®³æ¤œå‡ºé€Ÿåº¦ã¨è² è·ã®ãƒãƒ©ãƒ³ã‚¹ |

---

## 10. æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | ç›®æ¨™ | æ¸¬å®šæ–¹æ³• |
|-----------|-------------|------|---------|
| **æŠ•ç¨¿å“è³ªã‚¹ã‚³ã‚¢** | N/A | >= 3/5 å…¨æŠ•ç¨¿ | verifier.py |
| **X ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼** | 0 | 100 (æœˆæœ«) | X API |
| **TikTok ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼** | 0 | 100 (æœˆæœ«) | TikTok API |
| **ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡** | N/A | >= 5% | feedback-fetch |
| **LLMã‚³ã‚¹ãƒˆ** | 100% | 10-20% (memUå¾Œ) | OpenAI Usage |
| **ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒç‡** | N/A | 99.5% | Health check |
| **DLQã‚¨ãƒ³ãƒˆãƒªæ•°** | N/A | < 5/é€± | DLQç›£è¦– |

---

---

## 12. ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ (2026-02-05)

### 12.1 Tech Review: NEEDS_WORK

| Severity | Count | ä¸»è¦Issue |
|----------|-------|----------|
| **CRITICAL** | 2 | Slack webhookæœªã‚µãƒ‹ã‚¿ã‚¤ã‚ºã€JSON.parseç„¡é˜²å‚™ |
| **HIGH** | 4 | timedeltaæœªimportã€Race Conditionã€å…¥åŠ›æ¤œè¨¼ãªã—ã€APIåä¸é©åˆ‡ |
| **MEDIUM** | 6 | HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³æœªcloseã€DBã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã—ã€ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä¸çµ±ä¸€ |
| **LOW** | 4 | ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã€ãƒ­ã‚°ä¸çµ±ä¸€ã€bare except |

**P1ä¿®æ­£å¿…é ˆï¼ˆå®Ÿè£…å‰ï¼‰**:

1. **Slack Webhook ã‚µãƒ‹ã‚¿ã‚¤ã‚º**
```python
# main.py - sanitize hook content
def sanitize_for_slack(text: str, max_len: int = 50) -> str:
    # Remove potential injection vectors
    sanitized = re.sub(r'[<>{}]', '', text)
    return sanitized[:max_len] + '...' if len(sanitized) > max_len else sanitized
```

2. **JSON.parse é˜²å¾¡**
```javascript
// memuService.js
let extracted;
try {
  extracted = JSON.parse(response.choices[0].message.content);
} catch (e) {
  logger.warn('LLM returned invalid JSON, using empty array');
  extracted = { items: [] };
}
```

3. **Prisma Transactionè¿½åŠ **
```javascript
async function memorize(resource, userId) {
  return prisma.$transaction(async (tx) => {
    // ... all DB operations use tx
  });
}
```

4. **å…¥åŠ›æ¤œè¨¼è¿½åŠ **
```python
# hook_selector.py
def validate_hook(hook: Dict) -> bool:
    required = ['id', 'content', 'problemType']
    return all(hook.get(k) for k in required)

def select_hook_thompson(hooks: List[Dict], ...) -> Dict:
    valid_hooks = [h for h in hooks if validate_hook(h)]
    if not valid_hooks:
        raise ValueError("No valid hooks available")
    # ...
```

### 12.2 Architect Review: PASS

| Severity | Count | ä¸»è¦Concern |
|----------|-------|-------------|
| **CRITICAL** | 0 | - |
| **MEDIUM** | 2 | Embeddingæœªå®Ÿè£…ã€Categoryè¦ç´„ã‚³ã‚¹ãƒˆ |
| **LOW** | 5 | Session TTLã€Secret Rotationã€DLQæ°¸ç¶šåŒ– |

**ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è©•ä¾¡**:

| Scale | è©•ä¾¡ | å¯¾å¿œ |
|-------|------|------|
| 1xï¼ˆç¾åœ¨ï¼‰ | âœ… å•é¡Œãªã— | - |
| 10x | âš ï¸ è»½å¾®ãªæ‡¸å¿µ | Connection Poolã€ãƒãƒƒãƒå‡¦ç† |
| 100x | âŒ å†è¨­è¨ˆå¿…è¦ | Gatewayæ°´å¹³åˆ†æ•£ã€memUã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° |

**P1å®Ÿè£…å¿…é ˆï¼ˆv1å‰ï¼‰**:

1. **Embeddingç”Ÿæˆå®Ÿè£…**
```javascript
async function generateEmbedding(text) {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text.slice(0, 8000),
  });
  return response.data[0].embedding;
}
```

2. **ãƒãƒƒãƒCategoryè¦ç´„**
```javascript
// é–¾å€¤ãƒ™ãƒ¼ã‚¹ï¼ˆU7ã§æ±ºå®šæ¸ˆã¿ï¼‰: 20 Itemsè¶…éæ™‚ã®ã¿è¦ç´„
if (itemCount > 20) {
  await summarizeCategory(categoryName, userId);
}
```

### 12.3 Strengthsï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¯ãƒ¼è©•ä¾¡ï¼‰

| è©•ä¾¡å¯¾è±¡ | Tech | Arch |
|---------|------|------|
| ADRï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ±ºå®šè¨˜éŒ²ï¼‰ | âœ… | âœ… |
| Thompson Samplingå®Ÿè£… | âœ… Excellent | âœ… |
| Error Handlingï¼ˆEqual Jitter, DLQï¼‰ | âœ… | âœ… |
| Security Layers | âœ… | âœ… |
| Test Coverageï¼ˆ45ã‚·ãƒŠãƒªã‚ªï¼‰ | âœ… | âœ… |
| ä¸ç¢ºå®Ÿæ€§è§£æ±ºï¼ˆ18ä»¶ï¼‰ | âœ… | âœ… |

---

## 13. æ›´æ–°å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2026-02-03 | åˆç‰ˆä½œæˆï¼ˆ4ãƒªã‚µãƒ¼ãƒçµ±åˆï¼‰ |
| 2026-02-05 | v2: ä¸ç¢ºå®Ÿæ€§18ä»¶ã€ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª45ä»¶ã€å®Ÿè£…æ±ºå®šè¿½åŠ  |
| 2026-02-05 | Thompson Samplingå¼·åŒ–ï¼ˆDiscounted TS, Dynamic Prior, Novelty Bonusï¼‰ |
| 2026-02-05 | ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° 1-10 â†’ 0-5 ã«å¤‰æ›´ï¼ˆé–¾å€¤3ï¼‰ |
| 2026-02-05 | ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° Equal Jitter è¿½åŠ  |
| 2026-02-05 | DLQ 14æ—¥ä¿æŒãƒãƒªã‚·ãƒ¼è¿½åŠ  |
| 2026-02-05 | ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœè¿½åŠ ï¼ˆTech: NEEDS_WORK, Arch: PASSï¼‰ |

---

**END OF SPECIFICATION**
