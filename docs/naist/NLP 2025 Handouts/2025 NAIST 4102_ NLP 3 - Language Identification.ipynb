{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaOn1Go7NCSm"
   },
   "source": [
    "# Language Identifycation by Byte Language Model\n",
    "\n",
    "1. Copy this colab notebook into your google drive by clicking **Copy to Drive**. Check the [slides](https://docs.google.com/presentation/d/1MnY4LpI8oIodwVuvoSs-H-zmpmdN5iFu3NBvZ4Egsx8/edit?usp=sharing).\n",
    "2. You need background knowledge for [Python](https://www.python.org/) and [NumPy](https://numpy.org/).\n",
    "3. Run the cells yourself and tweak the code so that the byte-wise language model, i.e., `ByteLM`, works as expected.\n",
    "4. Identify the language for the test file `languages/unk.test` by using the code for byte-wise langauge model, i.e., `ByteLM`.\n",
    "5. Save this colab notebook as a **pdf** via **Print** in the file menu and submit it to https://edu-portal.naist.jp/ under **NLP #3** of **2025 NAIST 4102 NLP** using the report submission portal. Please make sure that **all the codes, execution results and your answers are visible** in the **pdf** for the assessment. If you violate the format requriement, then, **your score will be zero**. Check the [slides](https://docs.google.com/presentation/d/1MnY4LpI8oIodwVuvoSs-H-zmpmdN5iFu3NBvZ4Egsx8/edit?usp=sharing).\n",
    "6. Due date is **December 19th, 2025 JST**.\n",
    "\n",
    "For help regarding [Colab](https://colab.research.google.com/) or any technical issues, ask our TA, Ashmari Pramodya Pussewala Kankanange via <pussewala.ashmari.ow4@naist.ac.jp>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "x3X8DSpYMnFR"
   },
   "outputs": [],
   "source": [
    "#@markdown Please fill in your name, student id and email address.\n",
    "\n",
    "NAME = 'daisuke narita' #@param {type: 'string'}\n",
    "STUDENT_ID = '2411218' #@param {type: 'string'}\n",
    "EMAIL = 'narita.daisuke.nd4@naist.ac.jp' #@param {type: 'string'}\n",
    "\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4gtwt2NWyPN"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "We will give *70 points* for fixing a bug in the `ByteLM` class so that it can return perplexity values correctly, i.e., finite values, not, e.g., `Inf` when using the `perplexity` method.\n",
    "In addition, *30 points* will be credited when identifying the language of a test file, `languages/unk.test`. See each section for details.\n",
    "\n",
    "* You can use any external libraries so long as you don't break APIs as documented/commented in `ByteLM` class. They are indicated by \"DO NOT CHANGE\" etc.\n",
    "\n",
    "* When changing `ByteLM`, leave comments as a justification of how the bug was resolved in the corresponding code block.\n",
    "\n",
    "* When identifying the language of the test file, add your code as a justification in the corresponding code block with comments.\n",
    "\n",
    "* You need to run the code blocks, keep the results in the notebook and explain the results in text blocks, otherwise, it is impossible to make an assessment.\n",
    "\n",
    "* Make it sure to **submit your file in pdf** and not other formats, e.g., `.ipynb`.\n",
    "\n",
    "### Extras\n",
    "\n",
    "Those who tried \"unique methods\" will be given at most *10 points*. The uniqueness is determined whether a submission employs a smoothing method other students have not tried. If you tune any hyperparameters, please leave your experiments in this colab, e.g., your code and results. A unique method for hyperparameter tuning will be also count for the extra points.\n",
    "However, maximum *10 points* will be deducted when violating rules, e.g., changing part of the codes/APIs which should not be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xfi4Sd4kT3Kc"
   },
   "source": [
    "## Download datasets\n",
    "\n",
    "We will download the datasets to train and test your language models. The data in the `languages` directory has two subdirectories, `dev` and `devtest`. THe file name takes the form of `{ISO639-language-code}.{dev,devtest}` and you can find the list of ISO 639 language codes to map a three-letter code into its corresponding language name at [Wikipedia](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "khLcyFcco2H5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: gdown\n",
      "unzip:  cannot find or open languages.zip, languages.zip.zip or languages.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# Download the file to `/content` directory.\n",
    "!gdown 12CDdzmMuEInj0bqhMjlGLsONxZm4Tv6_\n",
    "\n",
    "\n",
    "# Unzip it.\n",
    "!unzip -o languages.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbM4x0Ht4JtZ"
   },
   "source": [
    "## Import libraries\n",
    "\n",
    "Adds necessary imports here if you want to use additional libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EwlCwsiGsFRF"
   },
   "outputs": [],
   "source": [
    "# Import libraries used in this colab. Adds more when necessary.\n",
    "import collections\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "# Import google.colab only if running in Colab environment\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    # Not running in Colab, files module not needed for this assignment\n",
    "    pass\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wt8sYGWUIqy"
   },
   "source": [
    "## Language model implementation\n",
    "\n",
    "`ByteLM` is a language model class which loads a training data, estimate parameters and test it on a file to compute byet-wise perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oMcNDJo-0eaE"
   },
   "outputs": [],
   "source": [
    "class ByteLM:\n",
    "  \"\"\"Byte language model.\n",
    "\n",
    "  This is a very naive language model, in which byte-wise ngram probabilities\n",
    "  are estimated by maximum-likelihood without considering an issue of\n",
    "  out-of-vocabulary.\n",
    "\n",
    "  You may want to tweak `__init__`, `initial_state` and `logprob` methods to\n",
    "  alleviate the problem. However, do not change `perplexity` in order to check\n",
    "  whether the model is implemented correctly. When changing part of the codes,\n",
    "  please try to make it readable by using appropriate variable names or adding\n",
    "  comments. Feel free to add additional methods, if necessary.\n",
    "\n",
    "  Usages:\n",
    "    ```python\n",
    "    lm = ByteLM(path/to/train/data)\n",
    "    perplexity, prob = lm.perplexity(path/to/test/data)\n",
    "    ```\n",
    "  \"\"\"\n",
    "\n",
    "  # DO NOT CHANGE BOS VALUE.\n",
    "  # 0 will never appear in a text, thus, used it as a special symbol for\n",
    "  # a beginning-of-sentence symbol, i.e., BOS.\n",
    "  BOS: int = 0\n",
    "\n",
    "  def __init__(self, filename: str, order: int=3) -> None:\n",
    "    \"\"\"Initializes `ByteLM`.\n",
    "\n",
    "    You can change the arguments for this method if necessary, e.g., adding\n",
    "    hyperparameters to this model.\n",
    "\n",
    "    Args:\n",
    "      filename: str, text file to train this language model.\n",
    "      order: int, the n-gram order that should be greater than 1.\n",
    "    \"\"\"\n",
    "    if order <= 1:\n",
    "      raise ValueError(f'`order` must be greater than 1: {order}')\n",
    "    self.order = order\n",
    "\n",
    "    # Collect n-gram counts. The dictionary comprises a key of tuple of\n",
    "    # integers, i.e., (n-1)-gram, and its associated value of 256-dimensonal\n",
    "    # vector, i.e., counts for the following chars.\n",
    "    ngram_counts = collections.defaultdict(lambda: np.zeros([256]))\n",
    "    with open(filename, 'br') as f:\n",
    "      for line in f:  # read as a byte string.\n",
    "        buffer = [self.BOS] + list(line)  # `buffer` is now a list of integers.\n",
    "        for n in range(1, self.order + 1):\n",
    "          for i in range(len(buffer) - n + 1):\n",
    "            ngram = buffer[i:i + n]\n",
    "            ngram_counts[tuple(ngram[:-1])][ngram[-1]] += 1\n",
    "\n",
    "    # Maximum likelihood estimate for language model with smoothing.\n",
    "    # BUG FIX: The original code assigned -inf for zero probabilities, which causes\n",
    "    # infinite perplexity when test data contains bytes not seen in training data.\n",
    "    # Solution: Apply Laplace smoothing (add-one smoothing) to ensure all probabilities\n",
    "    # are positive and sum to 1. This prevents -inf values in log probabilities.\n",
    "    self.ngrams: Dict[Tuple[int], np.ndarray] = {}\n",
    "    for context, counts in ngram_counts.items():\n",
    "      # Laplace smoothing: add 1 to each count, then normalize\n",
    "      # This ensures all 256 possible bytes have non-zero probability\n",
    "      smoothed_counts = counts + 1.0\n",
    "      probs = smoothed_counts / np.sum(smoothed_counts)\n",
    "      # Now all probabilities are positive, so log probabilities are finite\n",
    "      log_probs = np.log(probs)\n",
    "      self.ngrams[context] = log_probs\n",
    "\n",
    "  def initial_state(self) -> Any:\n",
    "    \"\"\"Returns an initial state for language model computation.\n",
    "\n",
    "    You can change the code in this method, but keep the API, e.g, input\n",
    "    arguments, so that `perplexity()` method works as expected.\n",
    "\n",
    "    Returns:\n",
    "      A state representation for log probabilities computation.\n",
    "    \"\"\"\n",
    "    return []\n",
    "\n",
    "  def logprob(self, state: Any, x: int) -> Tuple[np.ndarray, Any]:\n",
    "    \"\"\"Returns log probabilities for the current input byte.\n",
    "\n",
    "    You can change the code in this method, but keep the API, e.g, input\n",
    "    arguments, so that `perplexity()` method works as expected.\n",
    "\n",
    "    It is a naive method for backing off to lower order n-grams, and may not be\n",
    "    optimal for the lower perplexity.\n",
    "\n",
    "    Args:\n",
    "      state: A state to compute log probability.\n",
    "      x: int, the current byte to compute `p(y | state, x)`.\n",
    "    Returns:\n",
    "      A pair of (log_probs, next_state) where `log_probs` is `np.ndarray` of log\n",
    "      probabilities p(y | state, x) of all bytes y, and `next_state` is a new\n",
    "      state for the next log probability computation with a new input. Note that\n",
    "      `log_probs[y]` is equal to `log p(y | state, x)`,\n",
    "      `log_probs.shape == (256,)`, `np.exp(log_probs) >= 0` and\n",
    "      `np.sum(np.exp(log_probs)) == 1`.\n",
    "    \"\"\"\n",
    "    # Backoff to lower order when necessary.\n",
    "    state = (state + [x])[-self.order + 1:]\n",
    "    for i in range(len(state), 0, -1):\n",
    "       context = state[-i:]\n",
    "       assert len(context) < self.order\n",
    "       ret = self.ngrams.get(tuple(context), None)\n",
    "       if ret is not None:\n",
    "         return ret, context\n",
    "\n",
    "    # Backoff to unigram.\n",
    "    ret = self.ngrams.get((), None)\n",
    "    assert ret is not None\n",
    "    return ret, []\n",
    "\n",
    "  def perplexity(self, filename: str) -> Tuple[float, float]:\n",
    "    \"\"\"Computes perplexity for text data.\n",
    "\n",
    "    DO NOT CHANGE THE API OR CODE IN THIS METHOD.\n",
    "\n",
    "    Args:\n",
    "      filename: str, text file to compute perplexity.\n",
    "    Returns:\n",
    "      A pair (perplexity, prob) where `perplexity` is the perplexity computed\n",
    "      for `filename`. `prob` is the cumulative product of probabilities of all\n",
    "      the bytes in `filename` to verify that this language model is\n",
    "      probabilistic or not. `prob` should be close to 1, otherwise, this is not\n",
    "      a language model.\n",
    "    \"\"\"\n",
    "    # Cumulative log_prob for perplexity computation.\n",
    "    cumulative_log_prob = 0.0\n",
    "    # Verify the distribution so that this language model is probabilistic.\n",
    "    prob = 1.0\n",
    "    # Total number of bytes.\n",
    "    total_bytes = 0\n",
    "    with open(filename, 'br') as f:\n",
    "      for line in f:\n",
    "        state = self.initial_state()\n",
    "        prev_x = self.BOS\n",
    "        for x in line:\n",
    "          log_probs, state = self.logprob(state, prev_x)\n",
    "          assert log_probs.size == 256, f\"expected 256, got: {log_probs.size}\"\n",
    "          cumulative_log_prob += log_probs[x]\n",
    "\n",
    "          probs = np.exp(log_probs)\n",
    "          assert (probs >= 0).all(), \"expected greater than or equal to zero.\"\n",
    "          prob *= np.sum(probs)  # Sum of `probs` should be close to 1.\n",
    "\n",
    "          prev_x = x\n",
    "\n",
    "        total_bytes += len(line)\n",
    "\n",
    "    return np.exp(-cumulative_log_prob / total_bytes), prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSbRCBqpToid"
   },
   "source": [
    "## Test your code (70 points in total)\n",
    "\n",
    "Please run the following code block to report the perplexity of English data using the byte language models trained on English, Japanese and two variants of Chinese. Note that you need to modify `ByteLM` class to avoid errors, e.g., reporting `Inf` or non-probabilistic modeling. In addition, add comments in the `ByteLM`  code block explaining how the change resolves the bug (60 points).\n",
    "\n",
    "You will observe differerent perplexities using language models trained on different languages. Please explain the reason in the *\"Why perplexities are different?\"* section (10 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nIhSOEdJzqgd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English model: perplexity: 13.431301184867843 prob: 0.999999999997258\n",
      "Japanese model: perplexity: 196.09094718142185 prob: 0.9999999999997222\n",
      "Simplified Chiense model: perplexity: 155.13165142613286 prob: 0.9999999999964516\n",
      "Traditional Chiense model: perplexity: 182.54658837935466 prob: 1.0000000000032374\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train languages models for English, Japanese and two variants of Chinese. You\n",
    "# can change the arguments to `ByteLM`, e.g., additional arguments for better\n",
    "# hyperparameters.\n",
    "model_eng = ByteLM(\"languages/dev/eng.dev\")\n",
    "model_jpn = ByteLM(\"languages/dev/jpn.dev\")\n",
    "model_zho_simpl = ByteLM(\"languages/dev/zho_simpl.dev\")\n",
    "model_zho_trad = ByteLM(\"languages/dev/zho_trad.dev\")\n",
    "\n",
    "# DO NOT CHANGE THE FOLLOWING CODES.\n",
    "\n",
    "# Test on English test data.\n",
    "perp_eng, prob_eng = model_eng.perplexity(\"languages/devtest/eng.devtest\")\n",
    "perp_jpn, prob_jpn = model_jpn.perplexity(\"languages/devtest/eng.devtest\")\n",
    "perp_zho_simpl, prob_zho_simpl = model_zho_simpl.perplexity(\"languages/devtest/eng.devtest\")\n",
    "perp_zho_trad, prob_zho_trad = model_zho_trad.perplexity(\"languages/devtest/eng.devtest\")\n",
    "\n",
    "# Print out perplexity and the cumulative product of sum of probabilities for\n",
    "# each language model.\n",
    "print(f\"English model: perplexity: {perp_eng} prob: {prob_eng}\")\n",
    "print(f\"Japanese model: perplexity: {perp_jpn} prob: {prob_jpn}\")\n",
    "print(f\"Simplified Chiense model: perplexity: {perp_zho_simpl} prob: {prob_zho_simpl}\")\n",
    "print(f\"Traditional Chiense model: perplexity: {perp_zho_trad} prob: {prob_zho_trad}\")\n",
    "\n",
    "# Assertions to make sure the perplexities are finite.\n",
    "assert np.isfinite(perp_eng)\n",
    "assert np.isfinite(perp_jpn)\n",
    "assert np.isfinite(perp_zho_simpl)\n",
    "assert np.isfinite(perp_zho_trad)\n",
    "\n",
    "# Assertions to make sure the cumulative product of probabilities are close to\n",
    "# one.\n",
    "assert np.allclose(prob_eng, 1.0)\n",
    "assert np.allclose(prob_jpn, 1.0)\n",
    "assert np.allclose(prob_zho_simpl, 1.0)\n",
    "assert np.allclose(prob_zho_trad, 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkyTCxWXMKHi"
   },
   "source": [
    "### Why perplexities are different? (10 points)\n",
    "\n",
    "The perplexities differ because each language model is trained on different training data with different byte distributions:\n",
    "\n",
    "1. **English model (lowest perplexity ~13.43)**: The test data is English, so the English training data has byte patterns that match the test data well. The model can predict English text accurately.\n",
    "\n",
    "2. **Japanese model (high perplexity ~196.09)**: Japanese uses different character encodings (e.g., UTF-8 encoding of Japanese characters) and byte patterns that don't match English text. The model trained on Japanese data is \"surprised\" by English byte sequences.\n",
    "\n",
    "3. **Simplified Chinese (~155.13)**: Lower than Japanese but higher than English. Chinese characters are encoded differently than English, but there might be some byte overlap.\n",
    "\n",
    "4. **Traditional Chinese (~182.55)**: Similar to Simplified Chinese but slightly higher perplexity, possibly due to different character distributions.\n",
    "\n",
    "**Perplexity** measures how \"surprised\" the model is by the test data. Lower perplexity means the model predicted the test data better. Since the test data is English, the English model has the lowest perplexity, while models trained on other languages have higher perplexity because their byte distributions don't match English text patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jg8opr-gUVm2"
   },
   "source": [
    "## Identify the language of `languages/unk.test` (30 points in total)\n",
    "\n",
    "Use `ByteLM` class to identify the language of the file `languages/unk.test`. You can train langauge models for several langaugages located under `languages/dev/*.dev` using `ByteLM`. Note that each file name takes the form of `ISO639-language-code.dev` so that you can train a small byte language model for each language. Use the models to identify the language of `languages/unk.test` by running `perplexity` method. You can find the list of ISO 639 language codes to map a three-letter code into its corresponding language name at [Wikipedia](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes).\n",
    "\n",
    "Please add your code in the following block and run it as a justification to identify the language. Then, fill in the answer in the form, *\"Please fill in your answer.\"* (20 points) and explain how you predict the language in the *\"How you identify the language?\"* section (10 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jKTAPE7J6CY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102 language files\n",
      "Training models for all languages and testing on unk.test...\n",
      "\n",
      "afr            : perplexity =       43.365837, prob = 1.000000\n",
      "amh            : perplexity =      173.248665, prob = 1.000000\n",
      "ara            : perplexity =      288.650395, prob = 1.000000\n",
      "asm            : perplexity =      332.158087, prob = 1.000000\n",
      "ast            : perplexity =       22.095465, prob = 1.000000\n",
      "azj            : perplexity =       59.964178, prob = 1.000000\n",
      "bel            : perplexity =      357.242794, prob = 1.000000\n",
      "ben            : perplexity =      354.907909, prob = 1.000000\n",
      "bos            : perplexity =       46.668334, prob = 1.000000\n",
      "bul            : perplexity =      186.469220, prob = 1.000000\n",
      "cat            : perplexity =       17.709293, prob = 1.000000\n",
      "ceb            : perplexity =       44.384510, prob = 1.000000\n",
      "ces            : perplexity =       47.977121, prob = 1.000000\n",
      "ckb            : perplexity =      494.222132, prob = 1.000000\n",
      "cym            : perplexity =       49.475355, prob = 1.000000\n",
      "dan            : perplexity =       38.172881, prob = 1.000000\n",
      "deu            : perplexity =       42.891044, prob = 1.000000\n",
      "ell            : perplexity =      173.362843, prob = 1.000000\n",
      "eng            : perplexity =       29.045792, prob = 1.000000\n",
      "est            : perplexity =       45.948871, prob = 1.000000\n",
      "fas            : perplexity =      167.168345, prob = 1.000000\n",
      "fin            : perplexity =       56.186021, prob = 1.000000\n",
      "fra            : perplexity =       21.490091, prob = 1.000000\n",
      "ful            : perplexity =       44.109562, prob = 1.000000\n",
      "gle            : perplexity =       47.038770, prob = 1.000000\n",
      "glg            : perplexity =       22.400035, prob = 1.000000\n",
      "guj            : perplexity =      615.150874, prob = 1.000000\n",
      "hau            : perplexity =       63.777126, prob = 1.000000\n",
      "heb            : perplexity =      204.491384, prob = 1.000000\n",
      "hin            : perplexity =      268.076216, prob = 1.000000\n",
      "hrv            : perplexity =       46.780001, prob = 1.000000\n",
      "hun            : perplexity =       46.164212, prob = 1.000000\n",
      "hye            : perplexity =      233.871812, prob = 1.000000\n",
      "ibo            : perplexity =       71.092048, prob = 1.000000\n",
      "ind            : perplexity =       41.253625, prob = 1.000000\n",
      "isl            : perplexity =       59.527365, prob = 1.000000\n",
      "ita            : perplexity =       28.630081, prob = 1.000000\n",
      "jav            : perplexity =       42.408375, prob = 1.000000\n",
      "jpn            : perplexity =      214.162721, prob = 1.000000\n",
      "kam            : perplexity =       72.230553, prob = 1.000000\n",
      "kan            : perplexity =      260.322715, prob = 1.000000\n",
      "kat            : perplexity =      184.832744, prob = 1.000000\n",
      "kaz            : perplexity =      166.349858, prob = 1.000000\n",
      "kea            : perplexity =       36.160535, prob = 1.000000\n",
      "khm            : perplexity =      127.976635, prob = 1.000000\n",
      "kir            : perplexity =      175.050091, prob = 1.000000\n",
      "kor            : perplexity =      172.779452, prob = 1.000000\n",
      "lao            : perplexity =      121.603537, prob = 1.000000\n",
      "lav            : perplexity =       45.744352, prob = 1.000000\n",
      "lin            : perplexity =       61.419321, prob = 1.000000\n",
      "lit            : perplexity =       44.919621, prob = 1.000000\n",
      "ltz            : perplexity =       44.378936, prob = 1.000000\n",
      "lug            : perplexity =       69.871266, prob = 1.000000\n",
      "luo            : perplexity =       52.607662, prob = 1.000000\n",
      "mal            : perplexity =      281.727286, prob = 1.000000\n",
      "mar            : perplexity =      744.909439, prob = 1.000000\n",
      "mkd            : perplexity =      189.782003, prob = 1.000000\n",
      "mlt            : perplexity =       48.330232, prob = 1.000000\n",
      "mon            : perplexity =      517.063629, prob = 1.000000\n",
      "mri            : perplexity =      110.677151, prob = 1.000000\n",
      "msa            : perplexity =       45.118658, prob = 1.000000\n",
      "mya            : perplexity =      180.238216, prob = 1.000000\n",
      "nld            : perplexity =       40.163299, prob = 1.000000\n",
      "nob            : perplexity =       39.484132, prob = 1.000000\n",
      "npi            : perplexity =      204.523993, prob = 1.000000\n",
      "nso            : perplexity =       63.592166, prob = 1.000000\n",
      "nya            : perplexity =       60.412998, prob = 1.000000\n",
      "oci            : perplexity =       11.186163, prob = 1.000000\n",
      "orm            : perplexity =       68.696721, prob = 1.000000\n",
      "ory            : perplexity =      315.598924, prob = 1.000000\n",
      "pan            : perplexity =      206.947580, prob = 1.000000\n",
      "pol            : perplexity =       53.930710, prob = 1.000000\n",
      "por            : perplexity =       23.375723, prob = 1.000000\n",
      "pus            : perplexity =      120.170651, prob = 1.000000\n",
      "ron            : perplexity =       29.460511, prob = 1.000000\n",
      "rus            : perplexity =      171.850636, prob = 1.000000\n",
      "slk            : perplexity =       46.676436, prob = 1.000000\n",
      "slv            : perplexity =       46.988565, prob = 1.000000\n",
      "sna            : perplexity =       70.086991, prob = 1.000000\n",
      "snd            : perplexity =      336.531790, prob = 1.000000\n",
      "som            : perplexity =       60.174953, prob = 1.000000\n",
      "spa            : perplexity =       21.285006, prob = 1.000000\n",
      "srp            : perplexity =      207.721957, prob = 1.000000\n",
      "swe            : perplexity =       35.186089, prob = 1.000000\n",
      "swh            : perplexity =       67.604767, prob = 1.000000\n",
      "tam            : perplexity =      234.284892, prob = 1.000000\n",
      "tel            : perplexity =      175.778484, prob = 1.000000\n",
      "tgk            : perplexity =      174.181279, prob = 1.000000\n",
      "tgl            : perplexity =       49.092250, prob = 1.000000\n",
      "tha            : perplexity =      148.397924, prob = 1.000000\n",
      "tur            : perplexity =       50.761464, prob = 1.000000\n",
      "ukr            : perplexity =      267.418938, prob = 1.000000\n",
      "umb            : perplexity =       70.281511, prob = 1.000000\n",
      "urd            : perplexity =      252.230405, prob = 1.000000\n",
      "uzb            : perplexity =       54.873523, prob = 1.000000\n",
      "vie            : perplexity =       95.729624, prob = 1.000000\n",
      "wol            : perplexity =       45.616863, prob = 1.000000\n",
      "xho            : perplexity =       64.801210, prob = 1.000000\n",
      "yor            : perplexity =       61.671500, prob = 1.000000\n",
      "zho_simpl      : perplexity =      162.726595, prob = 1.000000\n",
      "zho_trad       : perplexity =      187.766051, prob = 1.000000\n",
      "zul            : perplexity =       77.459350, prob = 1.000000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "Best match (lowest perplexity): oci\n",
      "Perplexity: 11.186163\n",
      "Probability: 1.000000\n",
      "\n",
      "Top 5 languages (lowest perplexity):\n",
      "  1. oci            :       11.186163\n",
      "  2. cat            :       17.709293\n",
      "  3. spa            :       21.285006\n",
      "  4. fra            :       21.490091\n",
      "  5. ast            :       22.095465\n"
     ]
    }
   ],
   "source": [
    "# Add your code here to identfy the language of `languages/unk.test` and run it\n",
    "# as a justification.\n",
    "\n",
    "import os\n",
    "\n",
    "# Get all language files in the dev directory\n",
    "dev_dir = \"languages/dev\"\n",
    "language_files = [f for f in os.listdir(dev_dir) if f.endswith('.dev')]\n",
    "language_files.sort()\n",
    "\n",
    "print(f\"Found {len(language_files)} language files\")\n",
    "print(\"Training models for all languages and testing on unk.test...\")\n",
    "print()\n",
    "\n",
    "# Train models for all languages and compute perplexity\n",
    "results = []\n",
    "for lang_file in language_files:\n",
    "    lang_code = lang_file.replace('.dev', '')\n",
    "    try:\n",
    "        model = ByteLM(os.path.join(dev_dir, lang_file))\n",
    "        perp, prob = model.perplexity(\"languages/unk.test\")\n",
    "        results.append((lang_code, perp, prob))\n",
    "        print(f\"{lang_code:15s}: perplexity = {perp:15.6f}, prob = {prob:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {lang_code}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Find the language with the lowest perplexity\n",
    "if results:\n",
    "    results.sort(key=lambda x: x[1])  # Sort by perplexity\n",
    "    best_lang, best_perp, best_prob = results[0]\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Results Summary:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Best match (lowest perplexity): {best_lang}\")\n",
    "    print(f\"Perplexity: {best_perp:.6f}\")\n",
    "    print(f\"Probability: {best_prob:.6f}\")\n",
    "    print()\n",
    "    print(\"Top 5 languages (lowest perplexity):\")\n",
    "    for i, (lang, perp, prob) in enumerate(results[:5], 1):\n",
    "        print(f\"  {i}. {lang:15s}: {perp:15.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "id": "v_kAACSI0_NE"
   },
   "outputs": [],
   "source": [
    "#@markdown ###Please fill in your answer. (20 points)\n",
    "#@markdown You can find the list of ISO 639 language codes for mapping the\n",
    "#@markdown three-letter code into the language name at\n",
    "#@markdown [Wikipedia](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes).\n",
    "#@markdown You also need to write your code in the code block above and keep the\n",
    "#@markdown results of running the code.\n",
    "#@markdown \n",
    "#@markdown **Answer**: The language code is `oci` which corresponds to **Occitan**.\n",
    "#@markdown The test file `languages/unk.test` contains Occitan text, as evidenced by\n",
    "#@markdown the lowest perplexity value of 11.186163 when tested with the Occitan language model.\n",
    "\n",
    "LANGUAGE = 'Occitan' #@param {type: 'string'}\n",
    "\n",
    "#@markdown ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xBFpDHGJdWj"
   },
   "source": [
    "### How you identify the language? (10 points)\n",
    "\n",
    "I identified the language by training byte language models for all 102 available languages and computing perplexity on the test file `languages/unk.test`. The language model with the lowest perplexity is the best match because:\n",
    "\n",
    "1. **Perplexity measures prediction quality**: Lower perplexity means the model is less \"surprised\" by the test data, indicating that the training data (language) matches the test data better. Perplexity is calculated as the exponential of the average negative log-likelihood, so lower values indicate better predictions.\n",
    "\n",
    "2. **Byte language models capture statistical patterns**: Each language has characteristic byte patterns due to:\n",
    "   - Character encoding (e.g., UTF-8 encoding of characters)\n",
    "   - Character frequency distributions\n",
    "   - Common character sequences (n-grams)\n",
    "\n",
    "3. **The Occitan (oci) model achieved the lowest perplexity**: \n",
    "   - **Occitan (oci)**: perplexity = **11.186163** (lowest)\n",
    "   - Catalan (cat): perplexity = 17.709293\n",
    "   - Spanish (spa): perplexity = 21.285006\n",
    "   - French (fra): perplexity = 21.490091\n",
    "   - Asturian (ast): perplexity = 22.095465\n",
    "   \n",
    "   The Occitan model's perplexity (11.19) is significantly lower than all other languages, including closely related Romance languages. This strongly suggests that the test file contains Occitan text.\n",
    "\n",
    "4. **Supporting evidence**: The fact that other Romance languages (Catalan, Spanish, French, Asturian, Portuguese) also have relatively low perplexity (all in the top 5) supports this identification, as Occitan is a Romance language and shares similar byte patterns with other Romance languages. This linguistic relationship is reflected in the similar (though higher) perplexity values.\n",
    "\n",
    "5. **Methodology**: I trained a ByteLM model for each of the 102 languages in the `languages/dev/` directory, then computed perplexity on `languages/unk.test` for each model. The model with the lowest perplexity (Occitan) is the most likely language of the test file.\n",
    "\n",
    "The method is robust because:\n",
    "- It uses statistical properties of the entire text rather than specific keywords\n",
    "- It handles any language that can be represented as bytes\n",
    "- The Laplace smoothing ensures all bytes have non-zero probability, preventing infinite perplexity\n",
    "- It provides quantitative evidence (perplexity values) rather than subjective judgment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E5hTBiK-s5w"
   },
   "source": [
    "## Additional codes and logs for experiments\n",
    "\n",
    "Please add any experiemnts you have carried out. For example, you can run expeirments to find better hyperpaameters to train a byte language model. Or, you can investigate several methods to identify the language.\n",
    "\n",
    "Feel free to add additional code blocks if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "QY0t7uCN-8Mr"
   },
   "outputs": [],
   "source": [
    "# Add any codes to run your experiments, e.g., testing for hyperparameters.\n",
    "# You can use other data on `langauges/devtest`, e.g., `jpn.devtest`, to test\n",
    "# your codes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPwrcHc+QDQqvf0LxZd0lKL",
   "mount_file_id": "1US5LsIX_NKmKOYUBoiPMOuZVIcZkTjbm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
