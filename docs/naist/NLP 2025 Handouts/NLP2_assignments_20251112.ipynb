{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IVxL1Il54KU"
   },
   "source": [
    "# Assignments for Lexical Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install transformers torch requests beautifulsoup4 lxml numpy -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSb_7L2Q2j4y"
   },
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "t56MqfFgPDpS"
   },
   "outputs": [],
   "source": [
    "# Source: https://en.wikipedia.org/wiki/Large_language_model\n",
    "text0 = \"\"\"\n",
    "A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. The largest and most capable LLMs are generative pre-trained transformers (GPTs) and provide the core capabilities of chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\n",
    "They consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.\n",
    "LLMs evolved from earlier statistical and recurrent neural network approaches to language modeling. The transformer architecture, introduced in 2017, replaced recurrence with self-attention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. This innovation enabled models like GPT, BERT, and their successors, which demonstrated emergent behaviors at scale such as few-shot learning and compositional reasoning.\n",
    "Reinforcement learning, particularly policy gradient algorithms, has been adapted to fine-tune LLMs for desired behaviors beyond raw next-token prediction. Reinforcement learning from human feedback (RLHF) applies these methods to optimize a policy, the LLM's output distribution, against reward signals derived from human or automated preference judgments. This has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance.\n",
    "Benchmark evaluations for LLMs have evolved from narrow linguistic assessments toward comprehensive, multi-task evaluations measuring reasoning, factual accuracy, alignment, and safety. Hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements.\n",
    "\"\"\".strip(\"\\n\").replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zb5ScD_Q3SS6"
   },
   "source": [
    "### Assignments\n",
    "\n",
    "Answer the following questions regarding text0. Here, treat punctuation marks (\",\", \".\", \"(\", \")\") as regular words, and use whitespace as a separator (not as a token). Treat uppercase and lowercase letters as the same.\n",
    "1. Count the total number of word tokens.\n",
    "1. Count the number of word types (unique word tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AQ1Py-XTQh5y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 - Total number of word tokens: 386\n",
      "Question 2 - Number of word types (unique tokens): 232\n"
     ]
    }
   ],
   "source": [
    "text1 = text0.replace(\",\", \" ,\").replace(\".\", \" .\").replace(\"(\", \"( \").replace(\")\", \" )\")\n",
    "\n",
    "# Treat uppercase and lowercase letters in text1 as the same.\n",
    "text1_lower = text1.lower()\n",
    "\n",
    "# Count tokens in text1\n",
    "tokens = text1_lower.split()\n",
    "total_tokens = len(tokens)\n",
    "print(f\"Question 1 - Total number of word tokens: {total_tokens}\")\n",
    "\n",
    "# Count unique tokens in text1\n",
    "unique_tokens = set(tokens)\n",
    "num_types = len(unique_tokens)\n",
    "print(f\"Question 2 - Number of word types (unique tokens): {num_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFN20YPOdi4S"
   },
   "source": [
    "### Assignments\n",
    "\n",
    "Answer the following questions for the \"llm-jp-3.1-1.8b\" <https://huggingface.co/llm-jp/llm-jp-3.1-1.8b> tokenizer.\n",
    "\n",
    "3. Provide the vocabulary size of the tokenizer.\n",
    "4. Tokenize text0 with the tokenizer and count the total number of tokens.\n",
    "5. Tokenize text0 with the tokenizer and count the total number of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tFLdDXHFNJOh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3 - Vocabulary size: 99574\n",
      "Question 4 - Total number of tokens: 485\n",
      "Question 5 - Total number of unique tokens: 290\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llm-jp/llm-jp-3.1-1.8b\") # https://huggingface.co/llm-jp/llm-jp-3.1-1.8b\n",
    "\n",
    "# Confirm the vocabulary size of the tokenizer.\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(f\"Question 3 - Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Tokenize text0 with the tokenizer, and count the number of tokens.\n",
    "tokens = tokenizer.tokenize(text0)\n",
    "num_tokens = len(tokens)\n",
    "print(f\"Question 4 - Total number of tokens: {num_tokens}\")\n",
    "\n",
    "# Tokenize text0 with the tokenizer, and count the number of unique tokens.\n",
    "unique_tokens = set(tokens)\n",
    "num_unique_tokens = len(unique_tokens)\n",
    "print(f\"Question 5 - Total number of unique tokens: {num_unique_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj_w3T2pfHTp"
   },
   "source": [
    "### Assignments\n",
    "\n",
    "Answer the following questions for the \"Qwen/Qwen3-1.7B\" <https://huggingface.co/Qwen/Qwen3-1.7B> tokenizer.\n",
    "\n",
    "6. Provide the vocabulary size of the tokenizer.\n",
    "7. Tokenize text0 with the tokenizer and count the total number of tokens.\n",
    "8. Tokenize text0 with the tokenizer and Count the total number of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vcccGjWJNuxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6 - Vocabulary size: 151669\n",
      "Question 7 - Total number of tokens: 445\n",
      "Question 8 - Total number of unique tokens: 271\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\") # https://huggingface.co/Qwen/Qwen3-1.7B\n",
    "\n",
    "# Confirm the vocabulary size of the tokenizer.\n",
    "vocab_size_qwen = len(tokenizer.get_vocab())\n",
    "print(f\"Question 6 - Vocabulary size: {vocab_size_qwen}\")\n",
    "\n",
    "# Tokenize text0 with the tokenizer, and count the number of tokens.\n",
    "tokens_qwen = tokenizer.tokenize(text0)\n",
    "num_tokens_qwen = len(tokens_qwen)\n",
    "print(f\"Question 7 - Total number of tokens: {num_tokens_qwen}\")\n",
    "\n",
    "# Tokenize text0 with the tokenizer, and count the number of unique tokens.\n",
    "unique_tokens_qwen = set(tokens_qwen)\n",
    "num_unique_tokens_qwen = len(unique_tokens_qwen)\n",
    "print(f\"Question 8 - Total number of unique tokens: {num_unique_tokens_qwen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js1I0ycG679b"
   },
   "source": [
    "### Assignments\n",
    "\n",
    "Answer the following questions regarding papers published in ACL 2015 <https://aclanthology.org/events/acl-2015/> (a total of 363 papers) and ACL 2025 <https://aclanthology.org/events/acl-2025/> (a total of 4547 papers). Here, ACL 2015/2025 papers include workshop and Findings papers.\n",
    "\n",
    "9. Count the number of ACL 2015 and 2025 published whose titles contain \"tokenization\" or \"tokenisation\". Then, calculate the ratio of those papers among all publised papers. Finally, Choose one of the following:\n",
    "   - a. The numbers of tokenization papers are almost equivalent for both years, but the ratio is clearly higher for ACL 2025.\n",
    "   - b. The number of tokenization papers is larger for ACL 2025, but the ratio is almost equivalent for both years.\n",
    "   - c. The numbers of tokenization papers is larger for ACL 2025, and the ratio is clearly higher for ACL 2025.\n",
    "\n",
    "10. Count the number of ACL 2015 and 2025 papers whose titles contain \"morpheme\", \"morphology\", \"morphological\", \"word segmentation\", \"POS\", or \"POS tagging\", which roughly correspond to lexical analysis papers. Then, calculate the ratio of those papers among all publised papers. Finally, Choose one of the following:\n",
    "    - a. The numbers of lexical analysis papers is larger for ACL 2015, and the ratio is clearly higher for ACL 2015.\n",
    "    - b. The number of lexical analysis papers is larger for ACL 2025, but the ratio is almost equivalent for both years.\n",
    "    - c. The numbers of lexical analysis papers is larger for ACL 2025, but the ratio is clearly higher for ACL 2015.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "NH5fO-yy9lJL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ACL 2015 papers...\n",
      "Fetching ACL 2015 papers from https://aclanthology.org/events/acl-2015/...\n",
      "Found 363 titles for ACL 2015\n",
      "\n",
      "Fetching ACL 2025 papers...\n",
      "Fetching ACL 2025 papers from https://aclanthology.org/events/acl-2025/...\n",
      "Found 37 volume sections for ACL 2025\n",
      "Found 4809 papers in https://aclanthology.org/volumes/2025.acl-long/\n",
      "Found 294 papers in https://aclanthology.org/volumes/2025.acl-short/\n",
      "Found 195 papers in https://aclanthology.org/volumes/2025.acl-demo/\n",
      "Found 261 papers in https://aclanthology.org/volumes/2025.acl-srw/\n",
      "Found 27 papers in https://aclanthology.org/volumes/2025.acl-tutorials/\n",
      "Found 330 papers in https://aclanthology.org/volumes/2025.acl-industry/\n",
      "Found 4164 papers in https://aclanthology.org/volumes/2025.findings-acl/\n",
      "Found 3395 titles for ACL 2025\n",
      "\n",
      "[Question 9]\n",
      "ACL 2015 papers with 'tokenization' or 'tokenisation': 1\n",
      "ACL 2015 ratio: 0.0028 (1/363)\n",
      "ACL 2025 papers with 'tokenization' or 'tokenisation': 8\n",
      "ACL 2025 ratio: 0.0018 (8/4547)\n",
      "\n",
      "[Question 10]\n",
      "ACL 2015 papers with lexical analysis keywords: 5\n",
      "ACL 2015 ratio: 0.0138 (5/363)\n",
      "ACL 2025 papers with lexical analysis keywords: 10\n",
      "ACL 2025 ratio: 0.0022 (10/4547)\n",
      "\n",
      "[Answers]\n",
      "Question 9 answer: b\n",
      "Question 10 answer: c\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "def get_acl_papers(year):\n",
    "    \"\"\"ACL論文のタイトルを取得\"\"\"\n",
    "    url = f\"https://aclanthology.org/events/acl-{year}/\"\n",
    "    titles = []\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        print(f\"Fetching ACL {year} papers from {url}...\")\n",
    "        response = requests.get(url, headers=headers, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Method 1: Find all links with ACL paper ID pattern\n",
    "        # For older years: /P15-1001, /W15-1234\n",
    "        # For 2025: /2025.acl-long.1, /2025.findings-acl.1, etc.\n",
    "        if year == 2025:\n",
    "            # ACL 2025 uses different ID pattern\n",
    "            paper_links = soup.find_all('a', href=re.compile(r'/\\d{4}\\.[a-z-]+\\.\\d+'))\n",
    "        else:\n",
    "            paper_links = soup.find_all('a', href=re.compile(r'/[PWFD]\\d{2}-\\d{4}'))\n",
    "        \n",
    "        for link in paper_links:\n",
    "            title_text = link.get_text(strip=True)\n",
    "            if title_text and len(title_text) > 5 and title_text not in titles:\n",
    "                titles.append(title_text)\n",
    "        \n",
    "        # Method 2: Find span.entry-title\n",
    "        for span in soup.find_all('span', class_='entry-title'):\n",
    "            title_text = span.get_text(strip=True)\n",
    "            if title_text and len(title_text) > 5 and title_text not in titles:\n",
    "                titles.append(title_text)\n",
    "        \n",
    "        # Method 3: Find headings with paper links\n",
    "        if year == 2025:\n",
    "            paper_pattern = re.compile(r'/\\d{4}\\.[a-z-]+\\.\\d+')\n",
    "        else:\n",
    "            paper_pattern = re.compile(r'/[PWFD]\\d{2}-\\d{4}')\n",
    "        \n",
    "        for heading in soup.find_all(['h2', 'h3', 'h4', 'h5']):\n",
    "            link = heading.find('a', href=paper_pattern)\n",
    "            if link:\n",
    "                title_text = link.get_text(strip=True)\n",
    "                if title_text and len(title_text) > 5 and title_text not in titles:\n",
    "                    titles.append(title_text)\n",
    "        \n",
    "        # Method 4: For ACL 2025, try to get papers from volume sections and volume pages\n",
    "        if year == 2025:\n",
    "            # Find volume sections by ID\n",
    "            volume_ids = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link.get('href', '')\n",
    "                if href.startswith('#2025'):\n",
    "                    vol_id = href.lstrip('#')\n",
    "                    if vol_id not in volume_ids:\n",
    "                        volume_ids.append(vol_id)\n",
    "            \n",
    "            print(f\"Found {len(volume_ids)} volume sections for ACL 2025\")\n",
    "            \n",
    "            # Try to get papers from each volume section\n",
    "            for vol_id in volume_ids:\n",
    "                section = soup.find(id=vol_id)\n",
    "                if section:\n",
    "                    vol_papers = section.find_all('a', href=paper_pattern)\n",
    "                    for link in vol_papers:\n",
    "                        title_text = link.get_text(strip=True)\n",
    "                        if title_text and len(title_text) > 5 and title_text not in titles:\n",
    "                            titles.append(title_text)\n",
    "            \n",
    "            # Also try to access volume pages directly for major volumes\n",
    "            volume_urls = [\n",
    "                \"https://aclanthology.org/volumes/2025.acl-long/\",\n",
    "                \"https://aclanthology.org/volumes/2025.acl-short/\",\n",
    "                \"https://aclanthology.org/volumes/2025.acl-demo/\",\n",
    "                \"https://aclanthology.org/volumes/2025.acl-srw/\",\n",
    "                \"https://aclanthology.org/volumes/2025.acl-tutorials/\",\n",
    "                \"https://aclanthology.org/volumes/2025.acl-industry/\",\n",
    "                \"https://aclanthology.org/volumes/2025.findings-acl/\",\n",
    "            ]\n",
    "            \n",
    "            for vol_url in volume_urls:\n",
    "                try:\n",
    "                    vol_response = requests.get(vol_url, headers=headers, timeout=60)\n",
    "                    if vol_response.status_code == 200:\n",
    "                        vol_soup = BeautifulSoup(vol_response.content, 'html.parser')\n",
    "                        vol_links = vol_soup.find_all('a', href=paper_pattern)\n",
    "                        print(f\"Found {len(vol_links)} papers in {vol_url}\")\n",
    "                        for link in vol_links:\n",
    "                            title_text = link.get_text(strip=True)\n",
    "                            if title_text and len(title_text) > 5 and title_text not in titles:\n",
    "                                titles.append(title_text)\n",
    "                        time.sleep(0.5)  # Be polite\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching volume {vol_url}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Method 5: Try to find papers in list items or paragraphs\n",
    "        for elem in soup.find_all(['li', 'p', 'div']):\n",
    "            link = elem.find('a', href=paper_pattern)\n",
    "            if link:\n",
    "                title_text = link.get_text(strip=True)\n",
    "                if title_text and len(title_text) > 5 and title_text not in titles:\n",
    "                    titles.append(title_text)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        titles = list(dict.fromkeys(titles))\n",
    "        print(f\"Found {len(titles)} titles for ACL {year}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching ACL {year} papers: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return titles\n",
    "\n",
    "def count_papers_with_keywords(titles, keywords):\n",
    "    \"\"\"キーワードを含む論文数をカウント\"\"\"\n",
    "    matched_titles = set()\n",
    "    \n",
    "    for title in titles:\n",
    "        title_lower = title.lower()\n",
    "        for keyword in keywords:\n",
    "            matched = False\n",
    "            \n",
    "            if keyword.lower() == \"pos\":\n",
    "                # \"POS\" を独立した単語として検索（\"POS \"のようにスペースが後続、または文末、または句読点が後続）\n",
    "                pattern = r'\\bpos\\s+|\\bpos[.,;:)!?\\]\\}\\-]|\\bpos$'\n",
    "                if re.search(pattern, title_lower, re.IGNORECASE):\n",
    "                    matched = True\n",
    "            elif keyword.lower() == \"pos tagging\":\n",
    "                if \"pos tagging\" in title_lower:\n",
    "                    matched = True\n",
    "            else:\n",
    "                if keyword.lower() in title_lower:\n",
    "                    matched = True\n",
    "            \n",
    "            if matched:\n",
    "                matched_titles.add(title)\n",
    "                break\n",
    "    \n",
    "    return len(matched_titles)\n",
    "\n",
    "# ACL 2015と2025の論文を取得\n",
    "print(\"Fetching ACL 2015 papers...\")\n",
    "acl_2015_titles = get_acl_papers(2015)\n",
    "\n",
    "print(\"\\nFetching ACL 2025 papers...\")\n",
    "acl_2025_titles = get_acl_papers(2025)\n",
    "\n",
    "# 実際の論文数（問題文より）\n",
    "total_2015 = 363\n",
    "total_2025 = 4547  # 訂正後の値\n",
    "\n",
    "# Question 9: tokenization papers\n",
    "print(\"\\n[Question 9]\")\n",
    "keywords_tokenization = [\"tokenization\", \"tokenisation\"]\n",
    "count_2015_tokenization = count_papers_with_keywords(acl_2015_titles, keywords_tokenization)\n",
    "count_2025_tokenization = count_papers_with_keywords(acl_2025_titles, keywords_tokenization)\n",
    "\n",
    "ratio_2015_tokenization = count_2015_tokenization / total_2015 if total_2015 > 0 else 0\n",
    "ratio_2025_tokenization = count_2025_tokenization / total_2025 if total_2025 > 0 else 0\n",
    "\n",
    "print(f\"ACL 2015 papers with 'tokenization' or 'tokenisation': {count_2015_tokenization}\")\n",
    "print(f\"ACL 2015 ratio: {ratio_2015_tokenization:.4f} ({count_2015_tokenization}/{total_2015})\")\n",
    "print(f\"ACL 2025 papers with 'tokenization' or 'tokenisation': {count_2025_tokenization}\")\n",
    "print(f\"ACL 2025 ratio: {ratio_2025_tokenization:.4f} ({count_2025_tokenization}/{total_2025})\")\n",
    "\n",
    "# Question 10: lexical analysis papers\n",
    "print(\"\\n[Question 10]\")\n",
    "keywords_lexical = [\"morpheme\", \"morphology\", \"morphological\", \"word segmentation\", \"POS\", \"POS tagging\"]\n",
    "count_2015_lexical = count_papers_with_keywords(acl_2015_titles, keywords_lexical)\n",
    "count_2025_lexical = count_papers_with_keywords(acl_2025_titles, keywords_lexical)\n",
    "\n",
    "ratio_2015_lexical = count_2015_lexical / total_2015 if total_2015 > 0 else 0\n",
    "ratio_2025_lexical = count_2025_lexical / total_2025 if total_2025 > 0 else 0\n",
    "\n",
    "print(f\"ACL 2015 papers with lexical analysis keywords: {count_2015_lexical}\")\n",
    "print(f\"ACL 2015 ratio: {ratio_2015_lexical:.4f} ({count_2015_lexical}/{total_2015})\")\n",
    "print(f\"ACL 2025 papers with lexical analysis keywords: {count_2025_lexical}\")\n",
    "print(f\"ACL 2025 ratio: {ratio_2025_lexical:.4f} ({count_2025_lexical}/{total_2025})\")\n",
    "\n",
    "# 選択肢を決定\n",
    "print(\"\\n[Answers]\")\n",
    "# Question 9の選択肢\n",
    "if count_2025_tokenization > count_2015_tokenization and ratio_2025_tokenization > ratio_2015_tokenization * 1.1:\n",
    "    q9_choice = \"c\"\n",
    "elif count_2025_tokenization > count_2015_tokenization and abs(ratio_2025_tokenization - ratio_2015_tokenization) < 0.001:\n",
    "    q9_choice = \"b\"\n",
    "else:\n",
    "    q9_choice = \"a\"\n",
    "\n",
    "# Question 10の選択肢\n",
    "if count_2015_lexical > count_2025_lexical and ratio_2015_lexical > ratio_2025_lexical * 1.1:\n",
    "    q10_choice = \"a\"\n",
    "elif count_2025_lexical > count_2015_lexical and abs(ratio_2025_lexical - ratio_2015_lexical) < 0.001:\n",
    "    q10_choice = \"b\"\n",
    "else:\n",
    "    q10_choice = \"c\"\n",
    "\n",
    "print(f\"Question 9 answer: {q9_choice}\")\n",
    "print(f\"Question 10 answer: {q10_choice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results written to: NLP2_report.txt\n",
      "\n",
      "Summary:\n",
      "Question 1: 386\n",
      "Question 2: 232\n",
      "Question 3: 99574\n",
      "Question 4: 485\n",
      "Question 5: 290\n",
      "Question 6: 151669\n",
      "Question 7: 445\n",
      "Question 8: 271\n",
      "Question 9: b\n",
      "Question 10: c\n"
     ]
    }
   ],
   "source": [
    "# Write results to TXT file\n",
    "output_file = \"NLP2_report.txt\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"Student ID: 2411218\\n\")\n",
    "    f.write(f\"Name: daisuke narita\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"1,{total_tokens}\\n\")\n",
    "    f.write(f\"2,{num_types}\\n\")\n",
    "    f.write(f\"3,{vocab_size_llmjp}\\n\")\n",
    "    f.write(f\"4,{num_tokens_llmjp}\\n\")\n",
    "    f.write(f\"5,{num_unique_tokens_llmjp}\\n\")\n",
    "    f.write(f\"6,{vocab_size_qwen}\\n\")\n",
    "    f.write(f\"7,{num_tokens_qwen}\\n\")\n",
    "    f.write(f\"8,{num_unique_tokens_qwen}\\n\")\n",
    "    f.write(f\"9,{q9_choice}\\n\")\n",
    "    f.write(f\"10,{q10_choice}\\n\")\n",
    "\n",
    "print(f\"\\nResults written to: {output_file}\")\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Question 1: {total_tokens}\")\n",
    "print(f\"Question 2: {num_types}\")\n",
    "print(f\"Question 3: {vocab_size_llmjp}\")\n",
    "print(f\"Question 4: {num_tokens_llmjp}\")\n",
    "print(f\"Question 5: {num_unique_tokens_llmjp}\")\n",
    "print(f\"Question 6: {vocab_size_qwen}\")\n",
    "print(f\"Question 7: {num_tokens_qwen}\")\n",
    "print(f\"Question 8: {num_unique_tokens_qwen}\")\n",
    "print(f\"Question 9: {q9_choice}\")\n",
    "print(f\"Question 10: {q10_choice}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "/v2/external/notebooks/intro.ipynb",
     "timestamp": 1730383597088
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
