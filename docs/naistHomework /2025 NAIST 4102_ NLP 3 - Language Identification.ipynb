{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1US5LsIX_NKmKOYUBoiPMOuZVIcZkTjbm","authorship_tag":"ABX9TyPwrcHc+QDQqvf0LxZd0lKL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Language Identifycation by Byte Language Model\n","\n","1. Copy this colab notebook into your google drive by clicking **Copy to Drive**. Check the [slides](https://docs.google.com/presentation/d/1MnY4LpI8oIodwVuvoSs-H-zmpmdN5iFu3NBvZ4Egsx8/edit?usp=sharing).\n","2. You need background knowledge for [Python](https://www.python.org/) and [NumPy](https://numpy.org/).\n","3. Run the cells yourself and tweak the code so that the byte-wise language model, i.e., `ByteLM`, works as expected.\n","4. Identify the language for the test file `languages/unk.test` by using the code for byte-wise langauge model, i.e., `ByteLM`.\n","5. Save this colab notebook as a **pdf** via **Print** in the file menu and submit it to https://edu-portal.naist.jp/ under **NLP #3** of **2025 NAIST 4102 NLP** using the report submission portal. Please make sure that **all the codes, execution results and your answers are visible** in the **pdf** for the assessment. If you violate the format requriement, then, **your score will be zero**. Check the [slides](https://docs.google.com/presentation/d/1MnY4LpI8oIodwVuvoSs-H-zmpmdN5iFu3NBvZ4Egsx8/edit?usp=sharing).\n","6. Due date is **December 19th, 2025 JST**.\n","\n","For help regarding [Colab](https://colab.research.google.com/) or any technical issues, ask our TA, Ashmari Pramodya Pussewala Kankanange via <pussewala.ashmari.ow4@naist.ac.jp>.\n","\n","\n"],"metadata":{"id":"eaOn1Go7NCSm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3X8DSpYMnFR","cellView":"form"},"outputs":[],"source":["#@markdown Please fill in your name, student id and email address.\n","\n","NAME = 'Your Name' #@param {type: 'string'}\n","STUDENT_ID = 'student-id-number' #@param {type: 'string'}\n","EMAIL = 'your-account-name@naist.ac.jp' #@param {type: 'string'}\n","\n","#@markdown ---"]},{"cell_type":"markdown","source":["## Instructions\n","\n","We will give *70 points* for fixing a bug in the `ByteLM` class so that it can return perplexity values correctly, i.e., finite values, not, e.g., `Inf` when using the `perplexity` method.\n","In addition, *30 points* will be credited when identifying the language of a test file, `languages/unk.test`. See each section for details.\n","\n","* You can use any external libraries so long as you don't break APIs as documented/commented in `ByteLM` class. They are indicated by \"DO NOT CHANGE\" etc.\n","\n","* When changing `ByteLM`, leave comments as a justification of how the bug was resolved in the corresponding code block.\n","\n","* When identifying the language of the test file, add your code as a justification in the corresponding code block with comments.\n","\n","* You need to run the code blocks, keep the results in the notebook and explain the results in text blocks, otherwise, it is impossible to make an assessment.\n","\n","* Make it sure to **submit your file in pdf** and not other formats, e.g., `.ipynb`.\n","\n","### Extras\n","\n","Those who tried \"unique methods\" will be given at most *10 points*. The uniqueness is determined whether a submission employs a smoothing method other students have not tried. If you tune any hyperparameters, please leave your experiments in this colab, e.g., your code and results. A unique method for hyperparameter tuning will be also count for the extra points.\n","However, maximum *10 points* will be deducted when violating rules, e.g., changing part of the codes/APIs which should not be modified."],"metadata":{"id":"p4gtwt2NWyPN"}},{"cell_type":"markdown","source":["## Download datasets\n","\n","We will download the datasets to train and test your language models. The data in the `languages` directory has two subdirectories, `dev` and `devtest`. THe file name takes the form of `{ISO639-language-code}.{dev,devtest}` and you can find the list of ISO 639 language codes to map a three-letter code into its corresponding language name at [Wikipedia](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes).\n"],"metadata":{"id":"Xfi4Sd4kT3Kc"}},{"cell_type":"code","source":["# Download the file to `/content` directory.\n","!gdown 12CDdzmMuEInj0bqhMjlGLsONxZm4Tv6_\n","\n","\n","# Unzip it.\n","!unzip -o languages.zip"],"metadata":{"id":"khLcyFcco2H5","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import libraries\n","\n","Adds necessary imports here if you want to use additional libraries."],"metadata":{"id":"mbM4x0Ht4JtZ"}},{"cell_type":"code","source":["# Import libraries used in this colab. Adds more when necessary.\n","import collections\n","from typing import Any, Dict, List, Tuple\n","\n","from google.colab import files\n","import numpy as np"],"metadata":{"id":"EwlCwsiGsFRF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Language model implementation\n","\n","`ByteLM` is a language model class which loads a training data, estimate parameters and test it on a file to compute byet-wise perplexity."],"metadata":{"id":"8wt8sYGWUIqy"}},{"cell_type":"code","source":["class ByteLM:\n","  \"\"\"Byte language model.\n","\n","  This is a very naive language model, in which byte-wise ngram probabilities\n","  are estimated by maximum-likelihood without considering an issue of\n","  out-of-vocabulary.\n","\n","  You may want to tweak `__init__`, `initial_state` and `logprob` methods to\n","  alleviate the problem. However, do not change `perplexity` in order to check\n","  whether the model is implemented correctly. When changing part of the codes,\n","  please try to make it readable by using appropriate variable names or adding\n","  comments. Feel free to add additional methods, if necessary.\n","\n","  Usages:\n","    ```python\n","    lm = ByteLM(path/to/train/data)\n","    perplexity, prob = lm.perplexity(path/to/test/data)\n","    ```\n","  \"\"\"\n","\n","  # DO NOT CHANGE BOS VALUE.\n","  # 0 will never appear in a text, thus, used it as a special symbol for\n","  # a beginning-of-sentence symbol, i.e., BOS.\n","  BOS: int = 0\n","\n","  def __init__(self, filename: str, order: int=3) -> None:\n","    \"\"\"Initializes `ByteLM`.\n","\n","    You can change the arguments for this method if necessary, e.g., adding\n","    hyperparameters to this model.\n","\n","    Args:\n","      filename: str, text file to train this language model.\n","      order: int, the n-gram order that should be greater than 1.\n","    \"\"\"\n","    if order <= 1:\n","      raise ValueError(f'`order` must be greater than 1: {order}')\n","    self.order = order\n","\n","    # Collect n-gram counts. The dictionary comprises a key of tuple of\n","    # integers, i.e., (n-1)-gram, and its associated value of 256-dimensonal\n","    # vector, i.e., counts for the following chars.\n","    ngram_counts = collections.defaultdict(lambda: np.zeros([256]))\n","    with open(filename, 'br') as f:\n","      for line in f:  # read as a byte string.\n","        buffer = [self.BOS] + list(line)  # `buffer` is now a list of integers.\n","        for n in range(1, self.order + 1):\n","          for i in range(len(buffer) - n + 1):\n","            ngram = buffer[i:i + n]\n","            ngram_counts[tuple(ngram[:-1])][ngram[-1]] += 1\n","\n","    # Maximum likelihood estimate for language model.\n","    self.ngrams: Dict[Tuple[int], np.ndarray] = {}\n","    for context, counts in ngram_counts.items():\n","      probs = counts / np.sum(counts)\n","      # Computes log probabilities, but assigns -inf for zero probabilities.\n","      log_probs = np.where(probs == 0.0, -np.inf, np.log(probs))\n","      self.ngrams[context] = log_probs\n","\n","  def initial_state(self) -> Any:\n","    \"\"\"Returns an initial state for language model computation.\n","\n","    You can change the code in this method, but keep the API, e.g, input\n","    arguments, so that `perplexity()` method works as expected.\n","\n","    Returns:\n","      A state representation for log probabilities computation.\n","    \"\"\"\n","    return []\n","\n","  def logprob(self, state: Any, x: int) -> Tuple[np.ndarray, Any]:\n","    \"\"\"Returns log probabilities for the current input byte.\n","\n","    You can change the code in this method, but keep the API, e.g, input\n","    arguments, so that `perplexity()` method works as expected.\n","\n","    It is a naive method for backing off to lower order n-grams, and may not be\n","    optimal for the lower perplexity.\n","\n","    Args:\n","      state: A state to compute log probability.\n","      x: int, the current byte to compute `p(y | state, x)`.\n","    Returns:\n","      A pair of (log_probs, next_state) where `log_probs` is `np.ndarray` of log\n","      probabilities p(y | state, x) of all bytes y, and `next_state` is a new\n","      state for the next log probability computation with a new input. Note that\n","      `log_probs[y]` is equal to `log p(y | state, x)`,\n","      `log_probs.shape == (256,)`, `np.exp(log_probs) >= 0` and\n","      `np.sum(np.exp(log_probs)) == 1`.\n","    \"\"\"\n","    # Backoff to lower order when necessary.\n","    state = (state + [x])[-self.order + 1:]\n","    for i in range(len(state), 0, -1):\n","       context = state[-i:]\n","       assert len(context) < self.order\n","       ret = self.ngrams.get(tuple(context), None)\n","       if ret is not None:\n","         return ret, context\n","\n","    # Backoff to unigram.\n","    ret = self.ngrams.get((), None)\n","    assert ret is not None\n","    return ret, []\n","\n","  def perplexity(self, filename: str) -> Tuple[float, float]:\n","    \"\"\"Computes perplexity for text data.\n","\n","    DO NOT CHANGE THE API OR CODE IN THIS METHOD.\n","\n","    Args:\n","      filename: str, text file to compute perplexity.\n","    Returns:\n","      A pair (perplexity, prob) where `perplexity` is the perplexity computed\n","      for `filename`. `prob` is the cumulative product of probabilities of all\n","      the bytes in `filename` to verify that this language model is\n","      probabilistic or not. `prob` should be close to 1, otherwise, this is not\n","      a language model.\n","    \"\"\"\n","    # Cumulative log_prob for perplexity computation.\n","    cumulative_log_prob = 0.0\n","    # Verify the distribution so that this language model is probabilistic.\n","    prob = 1.0\n","    # Total number of bytes.\n","    total_bytes = 0\n","    with open(filename, 'br') as f:\n","      for line in f:\n","        state = self.initial_state()\n","        prev_x = self.BOS\n","        for x in line:\n","          log_probs, state = self.logprob(state, prev_x)\n","          assert log_probs.size == 256, f\"expected 256, got: {log_probs.size}\"\n","          cumulative_log_prob += log_probs[x]\n","\n","          probs = np.exp(log_probs)\n","          assert (probs >= 0).all(), \"expected greater than or equal to zero.\"\n","          prob *= np.sum(probs)  # Sum of `probs` should be close to 1.\n","\n","          prev_x = x\n","\n","        total_bytes += len(line)\n","\n","    return np.exp(-cumulative_log_prob / total_bytes), prob"],"metadata":{"id":"oMcNDJo-0eaE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test your code (70 points in total)\n","\n","Please run the following code block to report the perplexity of English data using the byte language models trained on English, Japanese and two variants of Chinese. Note that you need to modify `ByteLM` class to avoid errors, e.g., reporting `Inf` or non-probabilistic modeling. In addition, add comments in the `ByteLM`  code block explaining how the change resolves the bug (60 points).\n","\n","You will observe differerent perplexities using language models trained on different languages. Please explain the reason in the *\"Why perplexities are different?\"* section (10 points)."],"metadata":{"id":"kSbRCBqpToid"}},{"cell_type":"code","source":["\n","# Train languages models for English, Japanese and two variants of Chinese. You\n","# can change the arguments to `ByteLM`, e.g., additional arguments for better\n","# hyperparameters.\n","model_eng = ByteLM(\"languages/dev/eng.dev\")\n","model_jpn = ByteLM(\"languages/dev/jpn.dev\")\n","model_zho_simpl = ByteLM(\"languages/dev/zho_simpl.dev\")\n","model_zho_trad = ByteLM(\"languages/dev/zho_trad.dev\")\n","\n","# DO NOT CHANGE THE FOLLOWING CODES.\n","\n","# Test on English test data.\n","perp_eng, prob_eng = model_eng.perplexity(\"languages/devtest/eng.devtest\")\n","perp_jpn, prob_jpn = model_jpn.perplexity(\"languages/devtest/eng.devtest\")\n","perp_zho_simpl, prob_zho_simpl = model_zho_simpl.perplexity(\"languages/devtest/eng.devtest\")\n","perp_zho_trad, prob_zho_trad = model_zho_trad.perplexity(\"languages/devtest/eng.devtest\")\n","\n","# Print out perplexity and the cumulative product of sum of probabilities for\n","# each language model.\n","print(f\"English model: perplexity: {perp_eng} prob: {prob_eng}\")\n","print(f\"Japanese model: perplexity: {perp_jpn} prob: {prob_jpn}\")\n","print(f\"Simplified Chiense model: perplexity: {perp_zho_simpl} prob: {prob_zho_simpl}\")\n","print(f\"Traditional Chiense model: perplexity: {perp_zho_trad} prob: {prob_zho_trad}\")\n","\n","# Assertions to make sure the perplexities are finite.\n","assert np.isfinite(perp_eng)\n","assert np.isfinite(perp_jpn)\n","assert np.isfinite(perp_zho_simpl)\n","assert np.isfinite(perp_zho_trad)\n","\n","# Assertions to make sure the cumulative product of probabilities are close to\n","# one.\n","assert np.allclose(prob_eng, 1.0)\n","assert np.allclose(prob_jpn, 1.0)\n","assert np.allclose(prob_zho_simpl, 1.0)\n","assert np.allclose(prob_zho_trad, 1.0)\n"],"metadata":{"id":"nIhSOEdJzqgd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Why perplexities are different? (10 points)\n","\n","Please add your explanation here."],"metadata":{"id":"AkyTCxWXMKHi"}},{"cell_type":"markdown","source":["## Identify the language of `languages/unk.test` (30 points in total)\n","\n","Use `ByteLM` class to identify the language of the file `languages/unk.test`. You can train langauge models for several langaugages located under `languages/dev/*.dev` using `ByteLM`. Note that each file name takes the form of `ISO639-language-code.dev` so that you can train a small byte language model for each language. Use the models to identify the language of `languages/unk.test` by running `perplexity` method. You can find the list of ISO 639 language codes to map a three-letter code into its corresponding language name at [Wikipedia](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes).\n","\n","Please add your code in the following block and run it as a justification to identify the language. Then, fill in the answer in the form, *\"Please fill in your answer.\"* (20 points) and explain how you predict the language in the *\"How you identify the language?\"* section (10 points)."],"metadata":{"id":"Jg8opr-gUVm2"}},{"cell_type":"code","source":["# Add your code here to identfy the language of `languages/unk.test` and run it\n","# as a justification.\n","\n","\n","\n"],"metadata":{"id":"jKTAPE7J6CY3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown ###Please fill in your answer. (20 points)\n","#@markdown You can find the list of ISO 639 language codes for mapping the\n","#@markdown three-letter code into the language name at\n","#@markdown [Wikipedia](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes).\n","#@markdown You also need to write your code in the code block above and keep the\n","#@markdown results of running the code.\n","\n","LANGUAGE = 'English' #@param {type: 'string'}\n","\n","#@markdown ---"],"metadata":{"cellView":"form","id":"v_kAACSI0_NE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### How you identify the language? (10 points)\n","\n","Please add your explanation here."],"metadata":{"id":"7xBFpDHGJdWj"}},{"cell_type":"markdown","source":["## Additional codes and logs for experiments\n","\n","Please add any experiemnts you have carried out. For example, you can run expeirments to find better hyperpaameters to train a byte language model. Or, you can investigate several methods to identify the language.\n","\n","Feel free to add additional code blocks if necessary.\n"],"metadata":{"id":"0E5hTBiK-s5w"}},{"cell_type":"code","source":["# Add any codes to run your experiments, e.g., testing for hyperparameters.\n","# You can use other data on `langauges/devtest`, e.g., `jpn.devtest`, to test\n","# your codes.\n","\n"],"metadata":{"id":"QY0t7uCN-8Mr"},"execution_count":null,"outputs":[]}]}